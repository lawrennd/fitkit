{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawrennd/economic-fitness/blob/main/reddit_user_word_complexity_2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "## Reddit user × word data: “fitness / complexity” analysis\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Downloads a sample of Reddit users from BigQuery and aggregates their comments into per-user text.\n",
        "- Builds a `user` $\\times$ `word` matrix and its *support* (analogous to `country` $\\times$ `product`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d66ba0d"
      },
      "source": [
        "### BigQuery Setup Instructions\n",
        "\n",
        "To run the BigQuery query cells in this notebook, you need to have a Google Cloud Project with the BigQuery API enabled and proper authentication setup.\n",
        "\n",
        "Here's a general guide:\n",
        "\n",
        "1.  **Google Cloud Account**: If you don't have one, sign up for a Google Cloud account. You might be eligible for a free trial.\n",
        "    *   [Sign up for Google Cloud](https://cloud.google.com/free)\n",
        "\n",
        "2.  **Create/Select a Project**: In the [Google Cloud Console](https://console.cloud.google.com/), create a new project or select an existing one.\n",
        "    *   Ensure that **billing is enabled** for your project, as BigQuery usage incurs costs (though often minimal for small queries, especially with the free tier).\n",
        "\n",
        "3.  **Enable the BigQuery API**: For your selected project, ensure the BigQuery API is enabled.\n",
        "    *   Go to the [API Library](https://console.cloud.google.com/apis/library) in the Cloud Console.\n",
        "    *   Search for \"BigQuery API\" and enable it if it's not already enabled.\n",
        "\n",
        "4.  **Authenticate Colab**: In your Colab environment, the `google.colab.auth.authenticate_user()` function (called in Cell 3) will handle the authentication process by prompting you to log in with your Google account. This provides the necessary credentials for BigQuery access.\n",
        "\n",
        "    Alternatively, if you are working locally or need specific application-default credentials, you might use the `gcloud` CLI:\n",
        "    ```bash\n",
        "    gcloud auth application-default login\n",
        "    ```\n",
        "\n",
        "Once these steps are complete, you should be able to run the BigQuery cells successfully!"
      ],
      "id": "0d66ba0d"
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "### 0) Setup\n",
        "\n",
        "You’ll need BigQuery credentials configured locally (e.g. `gcloud auth application-default login`) and permission to access the public dataset `fh-bigquery.reddit_comments`.\n",
        "\n",
        "If you don’t have BigQuery access, you can still run the later cells by loading a cached dataframe (see the caching cell below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "outputs": [],
      "source": [
        "# Core\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sparse matrices\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Text features\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class QueryConfig:\n",
        "    target_subreddit: str = \"datascience\"\n",
        "    start_suffix: str = \"2015_01\"\n",
        "    end_suffix: str = \"2015_03\"\n",
        "    max_authors: int = 200\n",
        "    max_rows: int = 200_000\n",
        "    min_comments_per_author: int = 20\n",
        "    max_docs_per_author: int = 2000\n",
        "\n",
        "\n",
        "cfg = QueryConfig()\n",
        "\n",
        "CACHE_DIR = \"data\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "CACHE_PATH = os.path.join(\n",
        "    CACHE_DIR,\n",
        "    f\"reddit_{cfg.target_subreddit}_{cfg.start_suffix}_{cfg.end_suffix}_authors{cfg.max_authors}_rows{cfg.max_rows}.parquet\",\n",
        ")\n",
        "\n",
        "print(\"Cache path:\", CACHE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "outputs": [],
      "source": [
        "# 1) Run the BigQuery query (or load cache)\n",
        "\n",
        "def load_or_query_reddit(cfg: QueryConfig, cache_path: str) -> pd.DataFrame:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(\"Loading cached dataframe…\")\n",
        "        df = pd.read_parquet(cache_path)\n",
        "        return df\n",
        "\n",
        "    print(\"No cache found. Querying BigQuery…\")\n",
        "    # imported here so the notebook still imports without GCP\n",
        "    from google.cloud import bigquery\n",
        "    from google.colab import auth\n",
        "    import subprocess\n",
        "\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # Automate project selection: Find a project with BigQuery enabled\n",
        "    print(\"Searching for a valid BigQuery project...\")\n",
        "    client = None\n",
        "\n",
        "    try:\n",
        "        # Get list of projects using gcloud\n",
        "        proc = subprocess.run(['gcloud', 'projects', 'list', '--format=value(projectId)'], capture_output=True, text=True)\n",
        "        projects = proc.stdout.strip().split('\\n')\n",
        "\n",
        "        for pid in projects:\n",
        "            if not pid: continue\n",
        "            try:\n",
        "                print(f\"Trying project: {pid}...\")\n",
        "                c = bigquery.Client(project=pid)\n",
        "                # Run a minimal test query to check permissions/API status\n",
        "                c.query(\"SELECT 1\").result()\n",
        "                print(f\"-> Success! Using project: {pid}\")\n",
        "                client = c\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"   Skipping {pid} (BigQuery not accessible): {e}\")\n",
        "\n",
        "        if client is None:\n",
        "             raise RuntimeError(\"Could not find any project with BigQuery enabled. Please enable the BigQuery API on at least one project in your Google Cloud Console.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Automatic project setup failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    QUERY = f\"\"\"\n",
        "DECLARE target_subreddit STRING DEFAULT \"{cfg.target_subreddit}\";\n",
        "DECLARE start_suffix STRING DEFAULT \"{cfg.start_suffix}\";\n",
        "DECLARE end_suffix   STRING DEFAULT \"{cfg.end_suffix}\";\n",
        "DECLARE max_authors  INT64  DEFAULT {cfg.max_authors};\n",
        "DECLARE max_rows     INT64  DEFAULT {cfg.max_rows};\n",
        "DECLARE min_comments_per_author INT64 DEFAULT {cfg.min_comments_per_author};\n",
        "DECLARE max_docs_per_author INT64 DEFAULT {cfg.max_docs_per_author};\n",
        "\n",
        "WITH base AS (\n",
        "  SELECT author, body\n",
        "  FROM `fh-bigquery.reddit_comments.*`\n",
        "  WHERE _TABLE_SUFFIX BETWEEN start_suffix AND end_suffix\n",
        "    AND subreddit = target_subreddit\n",
        "    AND author IS NOT NULL\n",
        "    AND author NOT IN (\"[deleted]\", \"[removed]\")\n",
        "    AND body IS NOT NULL\n",
        "  LIMIT max_rows\n",
        "),\n",
        "sampled_authors AS (\n",
        "  SELECT author\n",
        "  FROM base\n",
        "  GROUP BY author\n",
        "  HAVING COUNT(*) >= min_comments_per_author\n",
        "  ORDER BY RAND()\n",
        "  LIMIT max_authors\n",
        ")\n",
        "SELECT\n",
        "  b.author,\n",
        "  STRING_AGG(b.body, \"\\n\" ORDER BY LENGTH(b.body) DESC LIMIT max_docs_per_author) AS user_text,\n",
        "  COUNT(*) AS n_comments\n",
        "FROM base b\n",
        "JOIN sampled_authors a\n",
        "USING (author)\n",
        "GROUP BY author\n",
        "ORDER BY n_comments DESC\n",
        "\"\"\"\n",
        "\n",
        "    df = client.query(QUERY).to_dataframe()\n",
        "    print(\"Saving cache…\")\n",
        "    df.to_parquet(cache_path, index=False)\n",
        "    return df\n",
        "\n",
        "\n",
        "df = load_or_query_reddit(cfg, CACHE_PATH)\n",
        "print(df.head())\n",
        "print(\"N users:\", len(df))\n",
        "print(df[\"n_comments\"].describe())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e900be4"
      },
      "source": [
        "# List available Google Cloud projects to find the Project ID\n",
        "!gcloud projects list"
      ],
      "id": "8e900be4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "# 2) Build user × word matrix\n",
        "#\n",
        "# In the paper's language, we will treat the *support* as M_{uw} = 1{X_{uw} > 0}.\n",
        "# For the rank-2 extension to be identifiable, it's helpful to keep *counts* (not just binary).\n",
        "# If you prefer pure support-only (presence/absence), set BINARY=True.\n",
        "\n",
        "BINARY = False\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",\n",
        "    min_df=3,          # ignore words used by <3 users\n",
        "    max_features=5000, # keep it demo-friendly\n",
        "    binary=BINARY,\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(df[\"user_text\"].fillna(\"\"))\n",
        "X = X.astype(np.float64)\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "user_ids = df[\"author\"].astype(str).tolist()\n",
        "\n",
        "print(\"Users:\", X.shape[0], \"Vocab:\", X.shape[1], \"binary:\", BINARY)\n",
        "\n",
        "# Support mask (structural zeros off-support)\n",
        "M = X.copy()\n",
        "M.data = np.ones_like(M.data)\n",
        "\n",
        "# Basic margins (analogues of diversification and ubiquity)\n",
        "user_strength = np.asarray(X.sum(axis=1)).ravel()\n",
        "word_strength = np.asarray(X.sum(axis=0)).ravel()\n",
        "\n",
        "print(\"User strength:\", pd.Series(user_strength).describe())\n",
        "print(\"Word strength:\", pd.Series(word_strength).describe())\n",
        "\n",
        "# Filter out degenerate rows/cols (helps numerics)\n",
        "min_user_mass = 5\n",
        "min_word_mass = 5\n",
        "\n",
        "keep_users = user_strength >= min_user_mass\n",
        "keep_words = word_strength >= min_word_mass\n",
        "\n",
        "X = X[keep_users][:, keep_words]\n",
        "M = M[keep_users][:, keep_words]\n",
        "\n",
        "user_ids = [u for u, ok in zip(user_ids, keep_users) if ok]\n",
        "vocab = vocab[keep_words]\n",
        "\n",
        "user_strength = np.asarray(X.sum(axis=1)).ravel()\n",
        "word_strength = np.asarray(X.sum(axis=0)).ravel()\n",
        "\n",
        "print(\"After filtering -> Users:\", X.shape[0], \"Vocab:\", X.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "source": [
        "### 3) Baseline: 1D Pietronero Fitness–Complexity fixed point\n",
        "\n",
        "This is the usual nonlinear rank-1 fixed point on the **support matrix** \\(M\\) (binary incidence). We’ll compute it as a scalar reference, then move to the rank-2 extension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "def fitness_complexity_1d(M_bin: sp.spmatrix, n_iter: int = 200, tol: float = 1e-10):\n",
        "    \"\"\"Compute 1D Fitness–Complexity fixed point on binary incidence matrix M.\n",
        "\n",
        "    M_bin: scipy sparse matrix (n_users × n_words), entries in {0,1}\n",
        "\n",
        "    Returns:\n",
        "      F (n_users,), Q (n_words,)\n",
        "    \"\"\"\n",
        "    n_users, n_words = M_bin.shape\n",
        "    F = np.ones(n_users, dtype=float)\n",
        "    Q = np.ones(n_words, dtype=float)\n",
        "\n",
        "    M_csr = M_bin.tocsr()\n",
        "    M_csc = M_bin.tocsc()\n",
        "\n",
        "    for it in range(n_iter):\n",
        "        F_new = M_csr @ Q\n",
        "        F_new = np.maximum(F_new, 1e-12)\n",
        "        F_new = F_new / F_new.mean()\n",
        "\n",
        "        invF = 1.0 / F_new\n",
        "        denom = M_csc.T @ invF\n",
        "        denom = np.maximum(denom, 1e-12)\n",
        "        Q_new = 1.0 / denom\n",
        "        Q_new = Q_new / Q_new.mean()\n",
        "\n",
        "        delta = max(np.max(np.abs(F_new - F)), np.max(np.abs(Q_new - Q)))\n",
        "        F, Q = F_new, Q_new\n",
        "        if delta < tol:\n",
        "            print(f\"Converged in {it+1} iterations\")\n",
        "            break\n",
        "\n",
        "    return F, Q\n",
        "\n",
        "\n",
        "F1, Q1 = fitness_complexity_1d(M)\n",
        "\n",
        "word_scores_1d = pd.Series(Q1, index=vocab).sort_values(ascending=False)\n",
        "user_scores_1d = pd.Series(F1, index=user_ids).sort_values(ascending=False)\n",
        "\n",
        "print(\"Top 15 words by 1D complexity:\")\n",
        "print(word_scores_1d.head(15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "### 4) 2D Pietronero-style extension (vector fitness / complexity)\n",
        "\n",
        "Per `economic-fitness.tex`, the multidimensional extension can be written on the support as\n",
        "\n",
        "\\[\n",
        "\\mu_{uw} = M_{uw}\\,a_u\\,b_w\\,\\exp(u_u^\\top v_w),\\qquad u_u, v_w\\in\\mathbb{R}^2.\n",
        "\\]\n",
        "\n",
        "But to keep the interpretation “everything lives in fitness and complexity”, we will **absorb** \\(a,b\\) into *augmented* vectors (paper’s reparameterisation):\n",
        "\n",
        "\\[\n",
        "\\widetilde F_u = \\begin{bmatrix}\\log a_u\\\\ 1\\\\ u_u\\end{bmatrix},\n",
        "\\qquad\n",
        "\\widetilde Q_w = \\begin{bmatrix}1\\\\ \\log b_w\\\\ v_w\\end{bmatrix},\n",
        "\\qquad\n",
        "\\mu_{uw} = M_{uw}\\exp(\\widetilde F_u^\\top \\widetilde Q_w).\n",
        "\\]\n",
        "\n",
        "Algorithmically (same as the paper’s Sinkhorn/IPF story):\n",
        "\n",
        "- For **fixed** \\(u,v\\), we solve the diagonal scalings \\(a,b\\) via **IPF/Sinkhorn** on the masked kernel \\(K_{uw}=M_{uw}\\exp(u_u^\\top v_w)\\).\n",
        "- Then we update \\(u,v\\) to better explain the observed data on the support.\n",
        "\n",
        "The “2D” part here refers to the dimension of the extra coordinates \\(u_u\\) and \\(v_w\\) (rank-2 association), not the total dimension of \\(\\widetilde F,\\widetilde Q\\) (which is \\(2+2=4\\)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": [],
      "source": [
        "def sinkhorn_scale_sparse(K: sp.spmatrix, r: np.ndarray, c: np.ndarray, n_iter: int = 500, tol: float = 1e-9):\n",
        "    \"\"\"Diagonal scaling w = diag(A) K diag(B) to match marginals r,c.\n",
        "\n",
        "    K: nonnegative sparse matrix\n",
        "    r: desired row sums (n_rows,)\n",
        "    c: desired col sums (n_cols,)\n",
        "\n",
        "    Returns:\n",
        "      A (n_rows,), B (n_cols,)\n",
        "    \"\"\"\n",
        "    K_csr = K.tocsr()\n",
        "    K_csc = K.tocsc()\n",
        "\n",
        "    A = np.ones(K.shape[0], dtype=np.float64)\n",
        "    B = np.ones(K.shape[1], dtype=np.float64)\n",
        "\n",
        "    # Safety epsilons\n",
        "    eps = 1e-32\n",
        "\n",
        "    for it in range(n_iter):\n",
        "        # A <- r / (K B)\n",
        "        KB = K_csr @ B\n",
        "        KB = np.maximum(KB, eps)\n",
        "        A_new = r / KB\n",
        "\n",
        "        # B <- c / (K^T A)\n",
        "        KtA = K_csc.T @ A_new\n",
        "        KtA = np.maximum(KtA, eps)\n",
        "        B_new = c / KtA\n",
        "\n",
        "        # Check marginal residuals occasionally\n",
        "        if it % 25 == 0 or it == n_iter - 1:\n",
        "            w_row = A_new * (K_csr @ B_new)\n",
        "            w_col = B_new * (K_csc.T @ A_new)\n",
        "            err = max(np.max(np.abs(w_row - r)), np.max(np.abs(w_col - c)))\n",
        "            if err < tol:\n",
        "                A, B = A_new, B_new\n",
        "                break\n",
        "\n",
        "        A, B = A_new, B_new\n",
        "\n",
        "    return A, B\n",
        "\n",
        "\n",
        "def build_kernel_from_uv(M_bin: sp.spmatrix, U: np.ndarray, V: np.ndarray) -> sp.coo_matrix:\n",
        "    \"\"\"Kernel K = M * exp(U V^T) evaluated only on support of M.\"\"\"\n",
        "    M_coo = M_bin.tocoo()\n",
        "    dots = np.einsum(\"ik,ik->i\", U[M_coo.row], V[M_coo.col])\n",
        "    data = np.exp(np.clip(dots, -50, 50))\n",
        "    return sp.coo_matrix((data, (M_coo.row, M_coo.col)), shape=M_bin.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "def fit_vector_fitness_complexity(\n",
        "    X_counts: sp.spmatrix,\n",
        "    M_bin: sp.spmatrix,\n",
        "    k: int = 2,\n",
        "    margins: str = \"data\",  # \"data\" or \"uniform\"\n",
        "    n_outer: int = 60,\n",
        "    sinkhorn_iter: int = 300,\n",
        "    lr: float = 0.05,\n",
        "    reg: float = 1e-3,\n",
        "    seed: int = 0,\n",
        "):\n",
        "    \"\"\"Fit the vector fitness/complexity extension from `economic-fitness.tex`.\n",
        "\n",
        "    Base (rank-1 / separable) model on support:\n",
        "      mu_uw = M_uw * a_u * b_w\n",
        "\n",
        "    Rank-k extension on support:\n",
        "      mu_uw = M_uw * a_u * b_w * exp(U_u^T V_w)\n",
        "\n",
        "    Reparameterisation (paper): define augmented vectors in R^{k+2}\n",
        "      Ftilde_u = [log a_u, 1, U_u]\n",
        "      Qtilde_w = [1, log b_w, V_w]\n",
        "    so that\n",
        "      mu_uw = M_uw * exp(Ftilde_u^T Qtilde_w).\n",
        "\n",
        "    Returns:\n",
        "      U (n_users,k), V (n_words,k)\n",
        "      a (n_users,), b (n_words,)\n",
        "      Ftilde (n_users,k+2), Qtilde (n_words,k+2)\n",
        "      mu_support (coo)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n_users, n_words = X_counts.shape\n",
        "\n",
        "    # Choose marginals (paper: data margins natural for counts; uniform for support-only)\n",
        "    if margins == \"data\":\n",
        "        r = np.asarray(X_counts.sum(axis=1)).ravel().astype(np.float64)\n",
        "        c = np.asarray(X_counts.sum(axis=0)).ravel().astype(np.float64)\n",
        "        total = r.sum()\n",
        "        r = r / max(total, 1e-12)\n",
        "        c = c / max(total, 1e-12)\n",
        "    elif margins == \"uniform\":\n",
        "        r = np.full(n_users, 1.0 / n_users, dtype=np.float64)\n",
        "        c = np.full(n_words, 1.0 / n_words, dtype=np.float64)\n",
        "    else:\n",
        "        raise ValueError(\"margins must be 'data' or 'uniform'\")\n",
        "\n",
        "    # Initialize latent coordinates small\n",
        "    U = 0.01 * rng.standard_normal((n_users, k))\n",
        "    V = 0.01 * rng.standard_normal((n_words, k))\n",
        "\n",
        "    X_coo = X_counts.tocoo()\n",
        "    M_coo = M_bin.tocoo()\n",
        "\n",
        "    for t in range(n_outer):\n",
        "        # 1) given U,V: build kernel and solve diagonal scaling (Sinkhorn/IPF)\n",
        "        K = build_kernel_from_uv(M_bin, U, V)\n",
        "        a, b = sinkhorn_scale_sparse(K, r=r, c=c, n_iter=sinkhorn_iter, tol=1e-10)\n",
        "\n",
        "        # mu on support (same sparsity as M)\n",
        "        mu_data = a[M_coo.row] * b[M_coo.col] * K.data\n",
        "        mu_lookup = {(i, j): m for i, j, m in zip(M_coo.row, M_coo.col, mu_data)}\n",
        "\n",
        "        # 2) gradient step on U,V (Poisson log-likelihood on observed support)\n",
        "        dU = np.zeros_like(U)\n",
        "        dV = np.zeros_like(V)\n",
        "\n",
        "        for i, j, x in zip(X_coo.row, X_coo.col, X_coo.data):\n",
        "            mu = mu_lookup.get((i, j), 0.0)\n",
        "            mu = max(mu, 1e-32)\n",
        "            err = float(x) - float(mu)  # derivative wrt dot\n",
        "            dU[i] += err * V[j]\n",
        "            dV[j] += err * U[i]\n",
        "\n",
        "        dU -= reg * U\n",
        "        dV -= reg * V\n",
        "\n",
        "        U = U + lr * dU\n",
        "        V = V + lr * dV\n",
        "\n",
        "        if t % 10 == 0 or t == n_outer - 1:\n",
        "            ll = 0.0\n",
        "            for i, j, x in zip(X_coo.row, X_coo.col, X_coo.data):\n",
        "                mu = mu_lookup.get((i, j), 0.0)\n",
        "                mu = max(mu, 1e-32)\n",
        "                ll += float(x) * np.log(mu) - float(mu)\n",
        "            print(f\"iter={t:03d}  ll={ll:.3e}\")\n",
        "\n",
        "    # Final scaling and augmented vectors\n",
        "    K = build_kernel_from_uv(M_bin, U, V)\n",
        "    a, b = sinkhorn_scale_sparse(K, r=r, c=c, n_iter=sinkhorn_iter, tol=1e-10)\n",
        "\n",
        "    loga = np.log(np.maximum(a, 1e-300))\n",
        "    logb = np.log(np.maximum(b, 1e-300))\n",
        "\n",
        "    Ftilde = np.concatenate([loga[:, None], np.ones((n_users, 1)), U], axis=1)\n",
        "    Qtilde = np.concatenate([np.ones((n_words, 1)), logb[:, None], V], axis=1)\n",
        "\n",
        "    M_coo = M_bin.tocoo()\n",
        "    mu_data = a[M_coo.row] * b[M_coo.col] * build_kernel_from_uv(M_bin, U, V).data\n",
        "    mu_support = sp.coo_matrix((mu_data, (M_coo.row, M_coo.col)), shape=M_bin.shape)\n",
        "\n",
        "    return U, V, a, b, Ftilde, Qtilde, mu_support\n",
        "\n",
        "\n",
        "U2, V2, a2, b2, Ftilde2, Qtilde2, mu2 = fit_vector_fitness_complexity(\n",
        "    X_counts=X,\n",
        "    M_bin=M,\n",
        "    k=2,\n",
        "    margins=\"uniform\",  # try \"data\" as well\n",
        "    n_outer=60,\n",
        "    sinkhorn_iter=300,\n",
        "    lr=0.02,\n",
        "    reg=1e-3,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "# For a 2D analysis we typically visualize the *latent* coordinates V2 (the last k dims of Qtilde)\n",
        "word_xy = pd.DataFrame(V2, index=vocab, columns=[\"dim1\", \"dim2\"])\n",
        "word_xy[\"complexity_2d\"] = np.linalg.norm(V2, axis=1)\n",
        "\n",
        "# Full vector complexity objects (in R^{k+2})\n",
        "word_Qtilde = pd.DataFrame(Qtilde2, index=vocab, columns=[\"const\", \"log_b\", \"v1\", \"v2\"])\n",
        "\n",
        "print(\"Top 15 words by latent (v) norm:\")\n",
        "print(word_xy.sort_values(\"complexity_2d\", ascending=False).head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": [],
      "source": [
        "# Plot word embedding (dim1, dim2)\n",
        "\n",
        "plt.figure(figsize=(9, 7))\n",
        "plt.scatter(word_xy[\"dim1\"], word_xy[\"dim2\"], s=10, alpha=0.4)\n",
        "plt.axhline(0, color=\"black\", linewidth=0.5)\n",
        "plt.axvline(0, color=\"black\", linewidth=0.5)\n",
        "plt.title(f\"Word embedding from rank-2 Pietronero extension (margins=uniform, binary={BINARY})\")\n",
        "plt.xlabel(\"dim1\")\n",
        "plt.ylabel(\"dim2\")\n",
        "\n",
        "# Label a few most complex words\n",
        "top = word_xy.sort_values(\"complexity_2d\", ascending=False).head(20)\n",
        "for w, row in top.iterrows():\n",
        "    plt.text(row[\"dim1\"], row[\"dim2\"], w, fontsize=8)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "source": [
        "### Notes on “weighting” / marginals (what the paper says)\n",
        "\n",
        "`economic-fitness.tex` is explicit that after you define a support \\(\\mathcal{S}\\), the remaining scale degrees of freedom are fixed by choosing row/column marginals \\(r_u, c_w\\):\n",
        "\n",
        "- For **quantitative flows** \\(X\\): the natural choice is data margins \\(r_u=X_{u\\cdot}\\), \\(c_w=X_{\\cdot w}\\).\n",
        "- For **support-only** work: you can choose convenient margins (e.g. uniform) to remove size/volume information.\n",
        "\n",
        "In this notebook you can switch that choice by setting `margins=\"uniform\"` or `margins=\"data\"` in the rank-2 fit.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}