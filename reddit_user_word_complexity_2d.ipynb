{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawrennd/economic-fitness/blob/main/reddit_user_word_complexity_2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "## Reddit user × word data: “fitness / complexity” analysis\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Downloads a sample of Reddit users from BigQuery and aggregates their comments into per-user text.\n",
        "- Builds a `user` $\\times$ `word` matrix and its *support* (analogous to `country` $\\times$ `product`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d66ba0d"
      },
      "source": [
        "### BigQuery Setup Instructions\n",
        "\n",
        "To run the BigQuery query cells in this notebook, you need to have a Google Cloud Project with the BigQuery API enabled and proper authentication setup.\n",
        "\n",
        "Here's a general guide:\n",
        "\n",
        "1.  **Google Cloud Account**: If you don't have one, sign up for a Google Cloud account. You might be eligible for a free trial.\n",
        "    *   [Sign up for Google Cloud](https://cloud.google.com/free)\n",
        "\n",
        "2.  **Create/Select a Project**: In the [Google Cloud Console](https://console.cloud.google.com/), create a new project or select an existing one.\n",
        "    *   Ensure that **billing is enabled** for your project, as BigQuery usage incurs costs (though often minimal for small queries, especially with the free tier).\n",
        "\n",
        "3.  **Enable the BigQuery API**: For your selected project, ensure the BigQuery API is enabled.\n",
        "    *   Go to the [API Library](https://console.cloud.google.com/apis/library) in the Cloud Console.\n",
        "    *   Search for \"BigQuery API\" and enable it if it's not already enabled.\n",
        "\n",
        "4.  **Authenticate Colab**: In your Colab environment, the `google.colab.auth.authenticate_user()` function (called in Cell 3) will handle the authentication process by prompting you to log in with your Google account. This provides the necessary credentials for BigQuery access.\n",
        "\n",
        "    Alternatively, if you are working locally or need specific application-default credentials, you might use the `gcloud` CLI:\n",
        "    ```bash\n",
        "    gcloud auth application-default login\n",
        "    ```\n",
        "\n",
        "Once these steps are complete, you should be able to run the BigQuery cells successfully!"
      ],
      "id": "0d66ba0d"
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "### 0) Setup\n",
        "\n",
        "You’ll need BigQuery credentials configured locally (e.g. `gcloud auth application-default login`) and permission to access the public dataset `fh-bigquery.reddit_comments`.\n",
        "\n",
        "If you don’t have BigQuery access, you can still run the later cells by loading a cached dataframe (see the caching cell below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "outputs": [],
      "source": [
        "# Core\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sparse matrices\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Text features\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class QueryConfig:\n",
        "    # Config for Wikipedia query\n",
        "    max_authors: int = 1000\n",
        "    min_comments_per_author: int = 20\n",
        "    max_docs_per_author: int = 2000\n",
        "    # Optional: List of specific users to prioritize/include if they meet criteria\n",
        "    specific_users: Tuple[str] = field(default_factory=tuple)\n",
        "\n",
        "    # Legacy/Unused for Wikipedia\n",
        "    target_subreddit: str = \"datascience\"\n",
        "    start_suffix: str = \"2015_01\"\n",
        "    end_suffix: str = \"2015_03\"\n",
        "    max_rows: int = 200_000\n",
        "\n",
        "\n",
        "# Standard random sampling (no specific users)\n",
        "cfg = QueryConfig()\n",
        "\n",
        "CACHE_DIR = \"data\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Updated cache path for Wikipedia data (v4 - random sample)\n",
        "CACHE_PATH = os.path.join(\n",
        "    CACHE_DIR,\n",
        "    f\"wikipedia_authors{cfg.max_authors}_v4.parquet\",\n",
        ")\n",
        "\n",
        "print(\"Cache path:\", CACHE_PATH)"
      ],
      "metadata": {
        "id": "8wz3052FZuGT"
      },
      "id": "8wz3052FZuGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "from google.colab import auth\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data(cfg):\n",
        "    \"\"\"Generate dummy data if BigQuery fails.\"\"\"\n",
        "    print(\"Generating synthetic Wikipedia-like data...\")\n",
        "    rng = np.random.default_rng(24)\n",
        "    authors = [f\"user_{i}\" for i in range(cfg.max_authors)]\n",
        "\n",
        "    # A small vocabulary\n",
        "    vocab = [\"data\", \"science\", \"python\", \"learning\", \"machine\", \"big\", \"query\",\n",
        "             \"analysis\", \"matrix\", \"complexity\", \"fitness\", \"network\", \"plot\",\n",
        "             \"code\", \"algorithm\", \"statistics\", \"neural\", \"deep\", \"model\",\n",
        "             \"optimization\", \"linear\", \"algebra\", \"visualization\", \"mining\",\n",
        "             \"edit\", \"wiki\", \"page\", \"revision\", \"history\", \"link\"]\n",
        "\n",
        "    data = []\n",
        "    for author in authors:\n",
        "        # Random text length\n",
        "        n_words = rng.integers(50, 500)\n",
        "        words = rng.choice(vocab, size=n_words)\n",
        "        user_text = \" \".join(words)\n",
        "        n_comments = rng.integers(cfg.min_comments_per_author, 1000)\n",
        "        data.append({\"author\": author, \"user_text\": user_text, \"n_comments\": n_comments})\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def load_or_query_wikipedia(cfg: QueryConfig, cache_path: str) -> pd.DataFrame:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(\"Loading cached dataframe…\")\n",
        "        df = pd.read_parquet(cache_path)\n",
        "        return df\n",
        "\n",
        "    print(\"No cache found. Querying BigQuery (Wikipedia)…\")\n",
        "\n",
        "    try:\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        # Automate project selection\n",
        "        print(\"Searching for a valid BigQuery project...\")\n",
        "        client = None\n",
        "\n",
        "        try:\n",
        "            # Get list of projects using gcloud\n",
        "            proc = subprocess.run(['gcloud', 'projects', 'list', '--format=value(projectId)'], capture_output=True, text=True)\n",
        "            projects = proc.stdout.strip().split('\\n')\n",
        "\n",
        "            for pid in projects:\n",
        "                if not pid: continue\n",
        "                try:\n",
        "                    print(f\"Trying project: {pid}...\")\n",
        "                    c = bigquery.Client(project=pid)\n",
        "                    c.query(\"SELECT 1\").result()\n",
        "                    print(f\"-> Success! Using project: {pid}\")\n",
        "                    client = c\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"   Skipping {pid}: {e}\")\n",
        "\n",
        "            if client is None:\n",
        "                 raise RuntimeError(\"Could not find any project with BigQuery enabled.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Automatic project setup failed: {e}\")\n",
        "            raise e\n",
        "\n",
        "        # Query Wikipedia samples\n",
        "        # Clean comments: remove /* Section */ markers and HTML tags\n",
        "        # Filter out bots: exclude usernames ending in \"bot\"\n",
        "        QUERY = \"\"\"\n",
        "    WITH edits AS (\n",
        "      SELECT\n",
        "        contributor_username AS author,\n",
        "        REGEXP_REPLACE(comment, r'/\\\\*.*?\\\\*/|<[^>]+>', '') AS body\n",
        "      FROM `bigquery-public-data.samples.wikipedia`\n",
        "      WHERE\n",
        "        contributor_username IS NOT NULL\n",
        "        AND comment IS NOT NULL\n",
        "        AND NOT REGEXP_CONTAINS(LOWER(contributor_username), r'bot$')\n",
        "    ),\n",
        "    valid_edits AS (\n",
        "      SELECT author, body\n",
        "      FROM edits\n",
        "      WHERE LENGTH(TRIM(body)) > 10  -- Filter out empty or very short comments after cleaning\n",
        "    ),\n",
        "    sampled_authors AS (\n",
        "      SELECT author\n",
        "      FROM valid_edits\n",
        "      GROUP BY author\n",
        "      HAVING COUNT(*) >= @min_comments_per_author\n",
        "      ORDER BY RAND()\n",
        "      LIMIT @max_authors\n",
        "    )\n",
        "    SELECT\n",
        "      author,\n",
        "      ARRAY_TO_STRING(ARRAY_AGG(body ORDER BY LENGTH(body) DESC LIMIT @max_docs_per_author), '\\\\n') AS user_text,\n",
        "      COUNT(*) AS n_comments\n",
        "    FROM valid_edits\n",
        "    JOIN sampled_authors\n",
        "    USING (author)\n",
        "    GROUP BY author\n",
        "    ORDER BY n_comments DESC\n",
        "    \"\"\"\n",
        "\n",
        "        job_config = bigquery.QueryJobConfig(\n",
        "            query_parameters=[\n",
        "                bigquery.ScalarQueryParameter(\"max_authors\", \"INT64\", cfg.max_authors),\n",
        "                bigquery.ScalarQueryParameter(\"min_comments_per_author\", \"INT64\", cfg.min_comments_per_author),\n",
        "                bigquery.ScalarQueryParameter(\"max_docs_per_author\", \"INT64\", cfg.max_docs_per_author),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        df = client.query(QUERY, job_config=job_config).to_dataframe()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nBigQuery query failed: {e}\")\n",
        "        print(\"Falling back to synthetic data so you can continue the tutorial.\")\n",
        "        df = generate_synthetic_data(cfg)\n",
        "\n",
        "    print(\"Saving cache…\")\n",
        "    df.to_parquet(cache_path, index=False)\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "rxtpC0AjZ7pS"
      },
      "id": "rxtpC0AjZ7pS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Force a new cache path to ensure we query real data\n",
        "print(f\"Using cache path: {CACHE_PATH}\")\n",
        "\n",
        "df = load_or_query_wikipedia(cfg, CACHE_PATH)\n",
        "print(df.head())\n",
        "print(\"N users:\", len(df))\n",
        "print(df[\"n_comments\"].describe())"
      ],
      "metadata": {
        "id": "wVdp6ak7Z-bT"
      },
      "id": "wVdp6ak7Z-bT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "# 2) Build user × word matrix\n",
        "#\n",
        "# In the paper's language, we will treat the *support* as M_{uw} = 1{X_{uw} > 0}.\n",
        "# For the rank-2 extension to be identifiable, it's helpful to keep *counts* (not just binary).\n",
        "# If you prefer pure support-only (presence/absence), set BINARY=True.\n",
        "\n",
        "BINARY = False\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",\n",
        "    min_df=3,          # ignore words used by <3 users\n",
        "    max_features=5000, # keep it demo-friendly\n",
        "    binary=BINARY,\n",
        ")\n",
        "\n",
        "# df is loaded from the previous cell\n",
        "X = vectorizer.fit_transform(df[\"user_text\"].fillna(\"\"))\n",
        "X = X.astype(np.float64)\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "user_ids = df[\"author\"].astype(str).tolist()\n",
        "\n",
        "print(f\"Raw Matrix -> Users: {X.shape[0]}, Vocab: {X.shape[1]}, binary: {BINARY}\")\n",
        "\n",
        "# Support mask (structural zeros off-support)\n",
        "M = X.copy()\n",
        "M.data = np.ones_like(M.data)\n",
        "\n",
        "# Basic margins (analogues of diversification and ubiquity)\n",
        "user_strength = np.asarray(X.sum(axis=1)).ravel()\n",
        "word_strength = np.asarray(X.sum(axis=0)).ravel()\n",
        "\n",
        "print(\"User strength:\", pd.Series(user_strength).describe())\n",
        "print(\"Word strength:\", pd.Series(word_strength).describe())\n",
        "\n",
        "# Filter out degenerate rows/cols (helps numerics)\n",
        "min_user_mass = 5\n",
        "min_word_mass = 5\n",
        "\n",
        "keep_users = user_strength >= min_user_mass\n",
        "keep_words = word_strength >= min_word_mass\n",
        "\n",
        "X = X[keep_users][:, keep_words]\n",
        "M = M[keep_users][:, keep_words]\n",
        "\n",
        "user_ids = [u for u, ok in zip(user_ids, keep_users) if ok]\n",
        "vocab = vocab[keep_words]\n",
        "\n",
        "user_strength = np.asarray(X.sum(axis=1)).ravel()\n",
        "word_strength = np.asarray(X.sum(axis=0)).ravel()\n",
        "\n",
        "print(f\"After filtering -> Users: {X.shape[0]}, Vocab: {X.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "source": [
        "### 3) Baseline: 1D Pietronero Fitness–Complexity fixed point\n",
        "\n",
        "This is the usual nonlinear rank-1 fixed point on the **support matrix** \\(M\\) (binary incidence). We’ll compute it as a scalar reference, then move to the rank-2 extension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "def fitness_complexity(M_bin: sp.spmatrix, n_iter: int = 200, tol: float = 1e-10):\n",
        "    \"\"\"Compute Fitness–Complexity fixed point on binary incidence matrix M.\n",
        "\n",
        "    M_bin: scipy sparse matrix (n_users × n_words), entries in {0,1}\n",
        "\n",
        "    Returns:\n",
        "      F (n_users,), Q (n_words,)\n",
        "    \"\"\"\n",
        "    n_users, n_words = M_bin.shape\n",
        "    F = np.ones(n_users, dtype=float)\n",
        "    Q = np.ones(n_words, dtype=float)\n",
        "\n",
        "    M_csr = M_bin.tocsr()\n",
        "    M_csc = M_bin.tocsc()\n",
        "\n",
        "    history = {\"dF\": [], \"dQ\": []}\n",
        "\n",
        "    for it in range(n_iter):\n",
        "        F_new = M_csr @ Q\n",
        "        F_new = np.maximum(F_new, 1e-12)\n",
        "        F_new = F_new / F_new.mean()\n",
        "\n",
        "        invF = 1.0 / F_new\n",
        "        denom = M_csc.T @ invF\n",
        "        denom = np.maximum(denom, 1e-12)\n",
        "        Q_new = 1.0 / denom\n",
        "        Q_new = Q_new / Q_new.mean()\n",
        "\n",
        "        delta = max(np.max(np.abs(F_new - F)), np.max(np.abs(Q_new - Q)))\n",
        "        F, Q = F_new, Q_new\n",
        "\n",
        "        dF = np.max(np.abs(F_new - F))\n",
        "        dQ = np.max(np.abs(Q_new - Q))\n",
        "        history[\"dF\"].append(dF)\n",
        "        history[\"dQ\"].append(dQ)\n",
        "\n",
        "        if delta < tol:\n",
        "            print(f\"Converged in {it+1} iterations\")\n",
        "            break\n",
        "\n",
        "    return F, Q, history\n",
        "\n",
        "def compute_eci_pci(M_bin: sp.spmatrix):\n",
        "    \"\"\"Compute ECI/PCI from binary matrix using the standard spectral formulation.\n",
        "\n",
        "    Returns:\n",
        "      eci: pd.Series indexed by country\n",
        "      pci: pd.Series indexed by product\n",
        "\n",
        "    Notes:\n",
        "    - This uses the country-country matrix: C = (M/kc) (M^T/kp)\n",
        "    - The trivial eigenvector corresponds to eigenvalue 1; we use the 2nd largest.\n",
        "    \"\"\"\n",
        "    Mv = M_bin.toarray()\n",
        "    kc = Mv.sum(axis=1)\n",
        "    kp = Mv.sum(axis=0)\n",
        "\n",
        "    # avoid divide-by-zero: drop zero-degree nodes if any\n",
        "    keep_c = kc > 0\n",
        "    keep_p = kp > 0\n",
        "    Mv = Mv[keep_c][:, keep_p]\n",
        "    kc = kc[keep_c]\n",
        "    kp = kp[keep_p]\n",
        "\n",
        "    Dc_inv = np.diag(1.0 / kc)\n",
        "    Dp_inv = np.diag(1.0 / kp)\n",
        "\n",
        "    C = Dc_inv @ Mv @ Dp_inv @ Mv.T\n",
        "\n",
        "    # eigen-decomposition (symmetric)\n",
        "    evals, evecs = np.linalg.eigh(C)\n",
        "    order = np.argsort(evals)[::-1]\n",
        "    evals = evals[order]\n",
        "    evecs = evecs[:, order]\n",
        "\n",
        "    if evecs.shape[1] < 2:\n",
        "        raise ValueError(\"Not enough dimensions for ECI (need at least 2 eigenvectors).\")\n",
        "\n",
        "    eci_vec = evecs[:, 1]\n",
        "    # sign is arbitrary; fix by correlating with diversification (positive)\n",
        "    if np.corrcoef(eci_vec, kc)[0, 1] < 0:\n",
        "        eci_vec = -eci_vec\n",
        "\n",
        "    # PCI as projection back to products\n",
        "    pci_vec = Dp_inv @ Mv.T @ eci_vec\n",
        "\n",
        "    # standardize for convenience\n",
        "    eci = (eci_vec - eci_vec.mean()) / (eci_vec.std(ddof=0) + 1e-12)\n",
        "    pci = (pci_vec - pci_vec.mean()) / (pci_vec.std(ddof=0) + 1e-12)\n",
        "\n",
        "    return eci, pci\n",
        "\n",
        "def sinkhorn_masked(M_bin: sp.spmatrix, r: np.ndarray, c: np.ndarray, n_iter: int = 2000, tol: float = 1e-12) -> tuple[np.ndarray, np.ndarray, np.ndarray, dict]:\n",
        "    \"\"\"Sinkhorn scaling on a binary mask M with kernel K = M (uniform cost on support).\n",
        "\n",
        "    Finds u,v such that W = diag(u) K diag(v) has row sums r and col sums c.\n",
        "\n",
        "    Requires K to have support that makes (r,c) feasible.\n",
        "    \"\"\"\n",
        "    K = M_bin #.toarray()\n",
        "\n",
        "    u = np.ones(K.shape[0])\n",
        "    v = np.ones(K.shape[1])\n",
        "\n",
        "    history = {\"dr\": [], \"dc\": []}\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "        Ku = K @ v\n",
        "        u_new = r / (Ku + 1e-30)\n",
        "        Kt = K.T @ u_new\n",
        "        v_new = c / (Kt + 1e-30)\n",
        "\n",
        "        W = (u_new[:, None] * K) * v_new[None, :]\n",
        "        dr = np.max(np.abs(W.sum(axis=1) - r))\n",
        "        dc = np.max(np.abs(W.sum(axis=0) - c))\n",
        "        history[\"dr\"].append(dr)\n",
        "        history[\"dc\"].append(dc)\n",
        "\n",
        "        u, v = u_new, v_new\n",
        "        if max(dr, dc) < tol:\n",
        "            break\n",
        "\n",
        "    W = (u[:, None] * K) * v[None, :]\n",
        "    return u, v, W, history\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mt3C-llTch21"
      },
      "id": "mt3C-llTch21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F, Q, fc_hist = fitness_complexity(M)\n",
        "eci, pci = compute_eci_pci(M)\n",
        "\n",
        "# Sinkhorn scaling with simple marginals\n",
        "r = np.ones(M.shape[0])\n",
        "r = r / r.sum()\n",
        "c = np.ones(M.shape[1])\n",
        "c = c / c.sum()\n",
        "\n",
        "u, v, W, sk_hist = sinkhorn_masked(M, r=r, c=c)\n",
        "\n",
        "results_countries = pd.DataFrame({\n",
        "    \"Fitness\": F,\n",
        "    \"ECI\": eci.reindex(F.index),\n",
        "    \"diversification_kc\": M.sum(axis=1),\n",
        "}).sort_values(\"Fitness\", ascending=False)\n",
        "\n",
        "results_products = pd.DataFrame({\n",
        "    \"Complexity\": Q,\n",
        "    \"PCI\": pci.reindex(Q.index),\n",
        "    \"ubiquity_kp\": M.sum(axis=0),\n",
        "}).sort_values(\"Complexity\", ascending=False)\n",
        "\n",
        "word_scores_1d = pd.Series(Q, index=vocab).sort_values(ascending=False)\n",
        "user_scores_1d = pd.Series(F, index=user_ids).sort_values(ascending=False)\n",
        "\n",
        "print(\"Top 20 words by complexity:\")\n",
        "print(word_scores_1d.head(20))\n",
        "print(\"Top 20 users by fitness:\")\n",
        "print(user_scores_1d.head(20))"
      ],
      "metadata": {
        "id": "_qCTfuYnaQa1"
      },
      "id": "_qCTfuYnaQa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_scores_1d.head(15)"
      ],
      "metadata": {
        "id": "eQrn6EVFIHZ4"
      },
      "id": "eQrn6EVFIHZ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostics: convergence\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
        "ax[0].plot(fc_hist[\"dF\"], label=\"max |ΔF|\")\n",
        "ax[0].plot(fc_hist[\"dQ\"], label=\"max |ΔQ|\")\n",
        "ax[0].set_yscale(\"log\")\n",
        "ax[0].set_title(\"FC convergence\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(sk_hist[\"dr\"], label=\"max row marginal error\")\n",
        "ax[1].plot(sk_hist[\"dc\"], label=\"max col marginal error\")\n",
        "ax[1].set_yscale(\"log\")\n",
        "ax[1].set_title(\"Sinkhorn/IPF convergence\")\n",
        "ax[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Diagnostics: nestedness-like visualization (sort by Fitness/Complexity)\n",
        "M_sorted = M.loc[results_countries.index, results_products.index]\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.imshow(M_sorted.to_numpy(), aspect=\"auto\", interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.title(\"M sorted by Fitness (rows) and Complexity (cols)\")\n",
        "plt.xlabel(\"products\")\n",
        "plt.ylabel(\"countries\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Diagnostics: compare rankings\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(results_countries[\"ECI\"], results_countries[\"Fitness\"], s=15, alpha=0.7)\n",
        "plt.xlabel(\"ECI (standardized)\")\n",
        "plt.ylabel(\"Fitness\")\n",
        "plt.title(\"Countries: Fitness vs ECI\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y9id_wWbc2hg"
      },
      "id": "Y9id_wWbc2hg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9FqKvK0Vc4Zv"
      },
      "id": "9FqKvK0Vc4Zv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}