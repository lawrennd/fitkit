{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lawrennd/economic-fitness/blob/main/reddit_user_word_complexity_2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "0"
   },
   "source": [
    "## Wikipedia Editing Data: fitness / complexity analysis\n",
    "\n",
    "This notebook:\n",
    "\n",
    "- Downloads a sample of Wikipedia users from BigQuery and aggregates their edits into per-user text.\n",
    "- Builds a `user` $\\times$ `word` matrix and its *support* (analogous to `country` $\\times$ `product`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "0d66ba0d"
   },
   "source": [
    "### BigQuery Setup Instructions\n",
    "\n",
    "To run the BigQuery query cells in this notebook, you need to have a Google Cloud Project with the BigQuery API enabled and proper authentication setup.\n",
    "\n",
    "Here's a general guide:\n",
    "\n",
    "1.  **Google Cloud Account**: If you don't have one, sign up for a Google Cloud account. You might be eligible for a free trial.\n",
    "    *   [Sign up for Google Cloud](https://cloud.google.com/free)\n",
    "\n",
    "2.  **Create/Select a Project**: In the [Google Cloud Console](https://console.cloud.google.com/), create a new project or select an existing one.\n",
    "    *   Ensure that **billing is enabled** for your project, as BigQuery usage incurs costs (though often minimal for small queries, especially with the free tier).\n",
    "\n",
    "3.  **Enable the BigQuery API**: For your selected project, ensure the BigQuery API is enabled.\n",
    "    *   Go to the [API Library](https://console.cloud.google.com/apis/library) in the Cloud Console.\n",
    "    *   Search for \"BigQuery API\" and enable it if it's not already enabled.\n",
    "\n",
    "4.  **Authenticate Colab**: In your Colab environment, the `google.colab.auth.authenticate_user()` function (called in Cell 3) will handle the authentication process by prompting you to log in with your Google account. This provides the necessary credentials for BigQuery access.\n",
    "\n",
    "    Alternatively, if you are working locally or need specific application-default credentials, you might use the `gcloud` CLI:\n",
    "    ```bash\n",
    "    gcloud auth application-default login\n",
    "    ```\n",
    "\n",
    "Once these steps are complete, you should be able to run the BigQuery cells successfully!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "1"
   },
   "source": [
    "### 0) Setup\n",
    "\n",
    "You’ll need BigQuery credentials configured locally (e.g. `gcloud auth application-default login`) and permission to access the public dataset `fh-bigquery.reddit_comments`.\n",
    "\n",
    "If you don’t have BigQuery access, you can still run the later cells by loading a cached dataframe (see the caching cell below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sparse matrices\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Text features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "8wz3052FZuGT"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class QueryConfig:\n",
    "    # Config for Wikipedia query\n",
    "    max_authors: int = 1000\n",
    "    min_comments_per_author: int = 20\n",
    "    max_docs_per_author: int = 2000\n",
    "    # Optional: List of specific users to prioritize/include if they meet criteria\n",
    "    specific_users: Tuple[str] = field(default_factory=tuple)\n",
    "\n",
    "    # Legacy/Unused for Wikipedia\n",
    "    target_subreddit: str = \"datascience\"\n",
    "    start_suffix: str = \"2015_01\"\n",
    "    end_suffix: str = \"2015_03\"\n",
    "    max_rows: int = 200_000\n",
    "\n",
    "\n",
    "# Standard random sampling (no specific users)\n",
    "cfg = QueryConfig()\n",
    "\n",
    "CACHE_DIR = \"data\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Updated cache path for Wikipedia data (v4 - random sample)\n",
    "CACHE_PATH = os.path.join(\n",
    "    CACHE_DIR,\n",
    "    f\"wikipedia_authors{cfg.max_authors}_v4.parquet\",\n",
    ")\n",
    "\n",
    "print(\"Cache path:\", CACHE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "rxtpC0AjZ7pS"
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data(cfg):\n",
    "    \"\"\"Generate dummy data if BigQuery fails.\"\"\"\n",
    "    print(\"Generating synthetic Wikipedia-like data...\")\n",
    "    rng = np.random.default_rng(24)\n",
    "    authors = [f\"user_{i}\" for i in range(cfg.max_authors)]\n",
    "\n",
    "    # A small vocabulary\n",
    "    vocab = [\"data\", \"science\", \"python\", \"learning\", \"machine\", \"big\", \"query\",\n",
    "             \"analysis\", \"matrix\", \"complexity\", \"fitness\", \"network\", \"plot\",\n",
    "             \"code\", \"algorithm\", \"statistics\", \"neural\", \"deep\", \"model\",\n",
    "             \"optimization\", \"linear\", \"algebra\", \"visualization\", \"mining\",\n",
    "             \"edit\", \"wiki\", \"page\", \"revision\", \"history\", \"link\"]\n",
    "\n",
    "    data = []\n",
    "    for author in authors:\n",
    "        # Random text length\n",
    "        n_words = rng.integers(50, 500)\n",
    "        words = rng.choice(vocab, size=n_words)\n",
    "        user_text = \" \".join(words)\n",
    "        n_comments = rng.integers(cfg.min_comments_per_author, 1000)\n",
    "        data.append({\"author\": author, \"user_text\": user_text, \"n_comments\": n_comments})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_or_query_wikipedia(cfg: QueryConfig, cache_path: str) -> pd.DataFrame:\n",
    "    if os.path.exists(cache_path):\n",
    "        print(\"Loading cached dataframe…\")\n",
    "        df = pd.read_parquet(cache_path)\n",
    "        return df\n",
    "\n",
    "    print(\"No cache found. Querying BigQuery (Wikipedia)…\")\n",
    "\n",
    "    try:\n",
    "        auth.authenticate_user()\n",
    "\n",
    "        # Automate project selection\n",
    "        print(\"Searching for a valid BigQuery project...\")\n",
    "        client = None\n",
    "\n",
    "        try:\n",
    "            # Get list of projects using gcloud\n",
    "            proc = subprocess.run(['gcloud', 'projects', 'list', '--format=value(projectId)'], capture_output=True, text=True)\n",
    "            projects = proc.stdout.strip().split('\\n')\n",
    "\n",
    "            for pid in projects:\n",
    "                if not pid: continue\n",
    "                try:\n",
    "                    print(f\"Trying project: {pid}...\")\n",
    "                    c = bigquery.Client(project=pid)\n",
    "                    c.query(\"SELECT 1\").result()\n",
    "                    print(f\"-> Success! Using project: {pid}\")\n",
    "                    client = c\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"   Skipping {pid}: {e}\")\n",
    "\n",
    "            if client is None:\n",
    "                 raise RuntimeError(\"Could not find any project with BigQuery enabled.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Automatic project setup failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "        # Query Wikipedia samples\n",
    "        # Clean comments: remove /* Section */ markers and HTML tags\n",
    "        # Filter out bots: exclude usernames ending in \"bot\"\n",
    "        QUERY = \"\"\"\n",
    "    WITH edits AS (\n",
    "      SELECT\n",
    "        contributor_username AS author,\n",
    "        REGEXP_REPLACE(comment, r'/\\\\*.*?\\\\*/|<[^>]+>', '') AS body\n",
    "      FROM `bigquery-public-data.samples.wikipedia`\n",
    "      WHERE\n",
    "        contributor_username IS NOT NULL\n",
    "        AND comment IS NOT NULL\n",
    "        AND NOT REGEXP_CONTAINS(LOWER(contributor_username), r'bot$')\n",
    "    ),\n",
    "    valid_edits AS (\n",
    "      SELECT author, body\n",
    "      FROM edits\n",
    "      WHERE LENGTH(TRIM(body)) > 10  -- Filter out empty or very short comments after cleaning\n",
    "    ),\n",
    "    sampled_authors AS (\n",
    "      SELECT author\n",
    "      FROM valid_edits\n",
    "      GROUP BY author\n",
    "      HAVING COUNT(*) >= @min_comments_per_author\n",
    "      ORDER BY RAND()\n",
    "      LIMIT @max_authors\n",
    "    )\n",
    "    SELECT\n",
    "      author,\n",
    "      ARRAY_TO_STRING(ARRAY_AGG(body ORDER BY LENGTH(body) DESC LIMIT @max_docs_per_author), '\\\\n') AS user_text,\n",
    "      COUNT(*) AS n_comments\n",
    "    FROM valid_edits\n",
    "    JOIN sampled_authors\n",
    "    USING (author)\n",
    "    GROUP BY author\n",
    "    ORDER BY n_comments DESC\n",
    "    \"\"\"\n",
    "\n",
    "        job_config = bigquery.QueryJobConfig(\n",
    "            query_parameters=[\n",
    "                bigquery.ScalarQueryParameter(\"max_authors\", \"INT64\", cfg.max_authors),\n",
    "                bigquery.ScalarQueryParameter(\"min_comments_per_author\", \"INT64\", cfg.min_comments_per_author),\n",
    "                bigquery.ScalarQueryParameter(\"max_docs_per_author\", \"INT64\", cfg.max_docs_per_author),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        df = client.query(QUERY, job_config=job_config).to_dataframe()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nBigQuery query failed: {e}\")\n",
    "        print(\"Falling back to synthetic data so you can continue the tutorial.\")\n",
    "        df = generate_synthetic_data(cfg)\n",
    "\n",
    "    print(\"Saving cache…\")\n",
    "    df.to_parquet(cache_path, index=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "wVdp6ak7Z-bT"
   },
   "outputs": [],
   "source": [
    "# Force a new cache path to ensure we query real data\n",
    "print(f\"Using cache path: {CACHE_PATH}\")\n",
    "\n",
    "df = load_or_query_wikipedia(cfg, CACHE_PATH)\n",
    "print(df.head())\n",
    "print(\"N users:\", len(df))\n",
    "print(df[\"n_comments\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "# 2) Build user × word matrix\n",
    "#\n",
    "# In the paper's language, we will treat the *support* as M_{uw} = 1{X_{uw} > 0}.\n",
    "# For the rank-2 extension to be identifiable, it's helpful to keep *counts* (not just binary).\n",
    "# If you prefer pure support-only (presence/absence), set BINARY=True.\n",
    "\n",
    "BINARY = False\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    min_df=3,          # ignore words used by <3 users\n",
    "    max_features=5000, # keep it demo-friendly\n",
    "    binary=BINARY,\n",
    ")\n",
    "\n",
    "# df is loaded from the previous cell\n",
    "X = vectorizer.fit_transform(df[\"user_text\"].fillna(\"\"))\n",
    "X = X.astype(np.float64)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "user_ids = df[\"author\"].astype(str).tolist()\n",
    "\n",
    "print(f\"Raw Matrix -> Users: {X.shape[0]}, Vocab: {X.shape[1]}, binary: {BINARY}\")\n",
    "\n",
    "# Support mask (structural zeros off-support)\n",
    "M = X.copy()\n",
    "M.data = np.ones_like(M.data)\n",
    "\n",
    "# Basic margins (analogues of diversification and ubiquity)\n",
    "user_strength = np.asarray(X.sum(axis=1)).ravel()\n",
    "word_strength = np.asarray(X.sum(axis=0)).ravel()\n",
    "\n",
    "print(\"User strength:\", pd.Series(user_strength).describe())\n",
    "print(\"Word strength:\", pd.Series(word_strength).describe())\n",
    "\n",
    "# Filter out degenerate rows/cols (helps numerics)\n",
    "min_user_mass = 5\n",
    "min_word_mass = 5\n",
    "\n",
    "keep_users = user_strength >= min_user_mass\n",
    "keep_words = word_strength >= min_word_mass\n",
    "\n",
    "X = X[keep_users][:, keep_words]\n",
    "M = M[keep_users][:, keep_words]\n",
    "\n",
    "user_ids = [u for u, ok in zip(user_ids, keep_users) if ok]\n",
    "vocab = vocab[keep_words]\n",
    "\n",
    "user_strength = np.asarray(X.sum(axis=1)).ravel()\n",
    "word_strength = np.asarray(X.sum(axis=0)).ravel()\n",
    "\n",
    "print(f\"After filtering -> Users: {X.shape[0]}, Vocab: {X.shape[1]}\")\n",
    "\n",
    "# Labeled view for plotting and downstream helpers\n",
    "M_df = pd.DataFrame.sparse.from_spmatrix(M, index=user_ids, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "5"
   },
   "source": [
    "### 3) Baseline: 1D Pietronero Fitness–Complexity fixed point\n",
    "\n",
    "This is the usual nonlinear rank-1 fixed point on the **support matrix** \\(M\\) (binary incidence). We’ll compute it as a scalar reference, then move to the rank-2 extension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "def fitness_complexity(M_bin: sp.spmatrix, n_iter: int = 200, tol: float = 1e-10):\n",
    "    \"\"\"Compute Fitness–Complexity fixed point on binary incidence matrix M.\n",
    "\n",
    "    M_bin: scipy sparse matrix (n_users × n_words), entries in {0,1}\n",
    "\n",
    "    Returns:\n",
    "      F (n_users,), Q (n_words,)\n",
    "    \"\"\"\n",
    "    n_users, n_words = M_bin.shape\n",
    "    F = np.ones(n_users, dtype=float)\n",
    "    Q = np.ones(n_words, dtype=float)\n",
    "\n",
    "    M_csr = M_bin.tocsr()\n",
    "\n",
    "    history = {\"dF\": [], \"dQ\": []}\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        F_new = M_csr @ Q\n",
    "        F_new = np.maximum(F_new, 1e-12)\n",
    "        F_new = F_new / F_new.mean()\n",
    "\n",
    "        invF = 1.0 / F_new\n",
    "        denom = M_csr.T @ invF  # denom_p = sum_u M_{up}/F_u\n",
    "        denom = np.maximum(denom, 1e-12)\n",
    "        Q_new = 1.0 / denom\n",
    "        Q_new = Q_new / Q_new.mean()\n",
    "\n",
    "        dF = float(np.max(np.abs(F_new - F)))\n",
    "        dQ = float(np.max(np.abs(Q_new - Q)))\n",
    "        history[\"dF\"].append(dF)\n",
    "        history[\"dQ\"].append(dQ)\n",
    "\n",
    "        F, Q = F_new, Q_new\n",
    "\n",
    "        if max(dF, dQ) < tol:\n",
    "            print(f\"Converged in {it+1} iterations\")\n",
    "            break\n",
    "\n",
    "    return F, Q, history\n",
    "\n",
    "def compute_eci_pci(M_bin: sp.spmatrix):\n",
    "    \"\"\"Compute ECI/PCI from binary matrix using the standard spectral formulation.\n",
    "\n",
    "    Returns:\n",
    "      eci: pd.Series indexed by country\n",
    "      pci: pd.Series indexed by product\n",
    "\n",
    "    Notes:\n",
    "    - This uses the country-country matrix: C = (M/kc) (M^T/kp)\n",
    "    - The trivial eigenvector corresponds to eigenvalue 1; we use the 2nd largest.\n",
    "    \"\"\"\n",
    "    Mv = M_bin.toarray()\n",
    "    kc = Mv.sum(axis=1)\n",
    "    kp = Mv.sum(axis=0)\n",
    "\n",
    "    # avoid divide-by-zero: drop zero-degree nodes if any\n",
    "    keep_c = kc > 0\n",
    "    keep_p = kp > 0\n",
    "    Mv = Mv[keep_c][:, keep_p]\n",
    "    kc = kc[keep_c]\n",
    "    kp = kp[keep_p]\n",
    "\n",
    "    Dc_inv = np.diag(1.0 / kc)\n",
    "    Dp_inv = np.diag(1.0 / kp)\n",
    "\n",
    "    C = Dc_inv @ Mv @ Dp_inv @ Mv.T\n",
    "\n",
    "    # eigen-decomposition (symmetric)\n",
    "    evals, evecs = np.linalg.eigh(C)\n",
    "    order = np.argsort(evals)[::-1]\n",
    "    evals = evals[order]\n",
    "    evecs = evecs[:, order]\n",
    "\n",
    "    if evecs.shape[1] < 2:\n",
    "        raise ValueError(\"Not enough dimensions for ECI (need at least 2 eigenvectors).\")\n",
    "\n",
    "    eci_vec = evecs[:, 1]\n",
    "    # sign is arbitrary; fix by correlating with diversification (positive)\n",
    "    if np.corrcoef(eci_vec, kc)[0, 1] < 0:\n",
    "        eci_vec = -eci_vec\n",
    "\n",
    "    # PCI as projection back to products\n",
    "    pci_vec = Dp_inv @ Mv.T @ eci_vec\n",
    "\n",
    "    # standardize for convenience\n",
    "    eci = (eci_vec - eci_vec.mean()) / (eci_vec.std(ddof=0) + 1e-12)\n",
    "    pci = (pci_vec - pci_vec.mean()) / (pci_vec.std(ddof=0) + 1e-12)\n",
    "\n",
    "    return eci, pci\n",
    "\n",
    "def sinkhorn_masked(\n",
    "    M_bin: sp.spmatrix,\n",
    "    r: np.ndarray,\n",
    "    c: np.ndarray,\n",
    "    n_iter: int = 2000,\n",
    "    tol: float = 1e-12,\n",
    "    eps: float = 1e-30,\n",
    ") -> tuple[np.ndarray, np.ndarray, sp.spmatrix, dict]:\n",
    "    \"\"\"Sinkhorn/IPF scaling on a *support mask*.\n",
    "\n",
    "    We take K = 1{(u,w) is in support} and find scalings u,v such that\n",
    "\n",
    "      W = diag(u) K diag(v)\n",
    "\n",
    "    matches the desired marginals r (rows) and c (cols).\n",
    "\n",
    "    This implementation is **sparse-safe** (never densifies K).\n",
    "    \"\"\"\n",
    "    K = M_bin.tocsr()\n",
    "\n",
    "    r = np.asarray(r, dtype=float).ravel()\n",
    "    c = np.asarray(c, dtype=float).ravel()\n",
    "\n",
    "    if K.shape[0] != r.shape[0] or K.shape[1] != c.shape[0]:\n",
    "        raise ValueError(f\"Shape mismatch: K is {K.shape}, r is {r.shape}, c is {c.shape}.\")\n",
    "\n",
    "    # normalize if user provided unnormalized masses\n",
    "    rs = float(r.sum())\n",
    "    cs = float(c.sum())\n",
    "    if rs <= 0 or cs <= 0:\n",
    "        raise ValueError(\"r and c must have positive total mass\")\n",
    "    if abs(rs - cs) > 1e-12 * max(rs, cs):\n",
    "        c = c * (rs / cs)\n",
    "\n",
    "    # feasibility guard: positive mass on isolated nodes is impossible\n",
    "    row_deg = np.asarray(K.sum(axis=1)).ravel()\n",
    "    col_deg = np.asarray(K.sum(axis=0)).ravel()\n",
    "    if np.any((row_deg == 0) & (r > 0)):\n",
    "        raise ValueError(\"Infeasible (r>0 on a row with zero support)\")\n",
    "    if np.any((col_deg == 0) & (c > 0)):\n",
    "        raise ValueError(\"Infeasible (c>0 on a col with zero support)\")\n",
    "\n",
    "    u = np.ones(K.shape[0], dtype=float)\n",
    "    v = np.ones(K.shape[1], dtype=float)\n",
    "\n",
    "    history = {\"dr\": [], \"dc\": [], \"iters\": 0}\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        Kv = K @ v\n",
    "        u_new = r / np.maximum(Kv, eps)\n",
    "\n",
    "        KTu = K.T @ u_new\n",
    "        v_new = c / np.maximum(KTu, eps)\n",
    "\n",
    "        # marginal errors without forming W\n",
    "        r_hat = u_new * (K @ v_new)\n",
    "        c_hat = v_new * (K.T @ u_new)\n",
    "\n",
    "        dr = float(np.max(np.abs(r_hat - r)))\n",
    "        dc = float(np.max(np.abs(c_hat - c)))\n",
    "        history[\"dr\"].append(dr)\n",
    "        history[\"dc\"].append(dc)\n",
    "        history[\"iters\"] = it + 1\n",
    "\n",
    "        u, v = u_new, v_new\n",
    "\n",
    "        if max(dr, dc) < tol:\n",
    "            break\n",
    "\n",
    "    # Sparse-safe construction of W (avoid 1D broadcasting surprises)\n",
    "    W = sp.diags(u) @ K @ sp.diags(v)\n",
    "    return u, v, W, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "mt3C-llTch21"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "_qCTfuYnaQa1"
   },
   "outputs": [],
   "source": [
    "F, Q, fc_hist = fitness_complexity(M)\n",
    "eci, pci = compute_eci_pci(M)\n",
    "\n",
    "F_s = pd.Series(F, index=user_ids, name=\"Fitness\")\n",
    "Q_s = pd.Series(Q, index=vocab, name=\"Complexity\")\n",
    "eci_s = pd.Series(eci, index=user_ids, name=\"ECI\")\n",
    "pci_s = pd.Series(pci, index=vocab, name=\"PCI\")\n",
    "\n",
    "kc = pd.Series(np.asarray(M.sum(axis=1)).ravel(), index=user_ids, name=\"diversification_kc\")\n",
    "kp = pd.Series(np.asarray(M.sum(axis=0)).ravel(), index=vocab, name=\"ubiquity_kp\")\n",
    "\n",
    "# Sinkhorn/IPF scaling to build a feasible flow W on the support.\n",
    "# Note: uniform marginals are *not always feasible* on a sparse support mask.\n",
    "USE_UNIFORM_MARGINALS = False\n",
    "\n",
    "if USE_UNIFORM_MARGINALS:\n",
    "    r = np.ones(M.shape[0], dtype=float)\n",
    "    r = r / r.sum()\n",
    "    c = np.ones(M.shape[1], dtype=float)\n",
    "    c = c / c.sum()\n",
    "else:\n",
    "    # Always-feasible choice: marginals implied by edge-uniform mass on the support\n",
    "    r = kc.to_numpy(dtype=float)\n",
    "    r = r / r.sum()\n",
    "    c = kp.to_numpy(dtype=float)\n",
    "    c = c / c.sum()\n",
    "\n",
    "u, v, W, sk_hist = sinkhorn_masked(M, r=r, c=c)\n",
    "\n",
    "results_countries = pd.concat([F_s, eci_s, kc], axis=1).sort_values(\"Fitness\", ascending=False)\n",
    "results_products = pd.concat([Q_s, pci_s, kp], axis=1).sort_values(\"Complexity\", ascending=False)\n",
    "\n",
    "word_scores_1d = Q_s.sort_values(ascending=False)\n",
    "user_scores_1d = F_s.sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 words by complexity:\")\n",
    "print(word_scores_1d.head(20))\n",
    "print(\"Top 20 users by fitness:\")\n",
    "print(user_scores_1d.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "eQrn6EVFIHZ4"
   },
   "outputs": [],
   "source": [
    "user_scores_1d.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "88qb5n2le32l"
   },
   "source": [
    "## Flow-native visualisations (Sinkhorn/OT coupling) + ranked barcodes\n",
    "\n",
    "The objects we visualise here are:\n",
    "\n",
    "- binary support: `M` (country×product)\n",
    "- Sinkhorn/IPF scaling factors: `u`, `v` (dual variables)\n",
    "- coupling / feasible flow: `W` where `W = diag(u) * M * diag(v)` (on the support)\n",
    "\n",
    "To avoid “hairballs”, every flow plot below supports **top-k / top-edge filtering**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "Y9id_wWbc2hg"
   },
   "outputs": [],
   "source": [
    "# Diagnostics: convergence\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax[0].plot(fc_hist[\"dF\"], label=\"max |ΔF|\")\n",
    "ax[0].plot(fc_hist[\"dQ\"], label=\"max |ΔQ|\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].set_title(\"FC convergence\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(sk_hist[\"dr\"], label=\"max row marginal error\")\n",
    "ax[1].plot(sk_hist[\"dc\"], label=\"max col marginal error\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].set_title(\"Sinkhorn/IPF convergence\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnostics: nestedness-like visualization (sort by Fitness/Complexity)\n",
    "M_sorted = M_df.loc[results_countries.index, results_products.index]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(M_sorted.sparse.to_dense().to_numpy(), aspect=\"auto\", interpolation=\"nearest\", cmap=\"Greys\")\n",
    "plt.title(\"M sorted by Fitness (rows) and Complexity (cols)\")\n",
    "plt.xlabel(\"words\")\n",
    "plt.ylabel(\"users\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnostics: compare rankings\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(results_countries[\"ECI\"], results_countries[\"Fitness\"], s=15, alpha=0.7)\n",
    "plt.xlabel(\"ECI (standardized)\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.title(\"Countries: Fitness vs ECI\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "9FqKvK0Vc4Zv"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "\n",
    "\n",
    "def _to_flow_df(M: pd.DataFrame, W: sp.spmatrix | np.ndarray) -> pd.DataFrame:\n",
    "    if sp.issparse(W):\n",
    "        # sparse-safe: keep as sparse frame\n",
    "        return pd.DataFrame.sparse.from_spmatrix(W, index=M.index, columns=M.columns)\n",
    "\n",
    "    W_df = pd.DataFrame(W, index=M.index, columns=M.columns)\n",
    "    # keep strictly on support (in case numerical noise fills zeros)\n",
    "    return W_df.where(M.astype(bool), other=0.0)\n",
    "\n",
    "\n",
    "def _top_subset(W_df: pd.DataFrame, top_c: int = 20, top_p: int = 30, by: str = \"mass\") -> pd.DataFrame:\n",
    "    \"\"\"Return a filtered W_df restricted to top countries/products.\n",
    "\n",
    "    by:\n",
    "      - \"mass\": uses row/col sums of W_df\n",
    "      - \"fitness_complexity\": uses global Series F/Q if present\n",
    "    \"\"\"\n",
    "    if by == \"fitness_complexity\" and \"F\" in globals() and \"Q\" in globals():\n",
    "        c_idx = list(pd.Series(F).sort_values(ascending=False).index[:top_c])\n",
    "        p_idx = list(pd.Series(Q).sort_values(ascending=False).index[:top_p])\n",
    "    else:\n",
    "        c_idx = list(W_df.sum(axis=1).sort_values(ascending=False).index[:top_c])\n",
    "        p_idx = list(W_df.sum(axis=0).sort_values(ascending=False).index[:top_p])\n",
    "    return W_df.loc[c_idx, p_idx]\n",
    "\n",
    "\n",
    "def plot_circular_bipartite_flow(\n",
    "    W_df: pd.DataFrame,\n",
    "    max_edges: int = 350,\n",
    "    min_edge_mass: float | None = None,\n",
    "    color_by: str = \"country\",\n",
    "    title: str = \"Circular bipartite flow (line-weighted, filtered)\",\n",
    "):\n",
    "    \"\"\"Chord-style circular bipartite flow using Bezier curves.\n",
    "\n",
    "    Notes:\n",
    "    - This draws *curves* (not full ribbons) with linewidth ∝ w_cp.\n",
    "    - Filter to top edges to avoid hairballs.\n",
    "    \"\"\"\n",
    "    countries = list(W_df.index)\n",
    "    products = list(W_df.columns)\n",
    "\n",
    "    edges = (\n",
    "        W_df.stack()\n",
    "        .rename(\"w\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"level_0\": \"country\", \"level_1\": \"product\"})\n",
    "    )\n",
    "\n",
    "    edges = edges[edges[\"w\"] > 0].sort_values(\"w\", ascending=False)\n",
    "    if min_edge_mass is not None:\n",
    "        edges = edges[edges[\"w\"] >= float(min_edge_mass)]\n",
    "    edges = edges.head(max_edges)\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        print(\"No edges to plot after filtering.\")\n",
    "        return\n",
    "\n",
    "    # angles: countries on left semicircle, products on right semicircle\n",
    "    n_c, n_p = len(countries), len(products)\n",
    "    theta_c = np.linspace(np.pi / 2, 3 * np.pi / 2, n_c, endpoint=False)\n",
    "    theta_p = np.linspace(-np.pi / 2, np.pi / 2, n_p, endpoint=False)\n",
    "\n",
    "    def pol2cart(theta, r=1.0):\n",
    "        return np.array([r * np.cos(theta), r * np.sin(theta)])\n",
    "\n",
    "    pos_c = {c: pol2cart(theta_c[i]) for i, c in enumerate(countries)}\n",
    "    pos_p = {p: pol2cart(theta_p[j]) for j, p in enumerate(products)}\n",
    "\n",
    "    # colors\n",
    "    if color_by == \"product\":\n",
    "        cmap = plt.get_cmap(\"tab20\")\n",
    "        colors = {p: cmap(i % 20) for i, p in enumerate(products)}\n",
    "        edge_color = lambda row: colors[row[\"product\"]]\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab20\")\n",
    "        colors = {c: cmap(i % 20) for i, c in enumerate(countries)}\n",
    "        edge_color = lambda row: colors[row[\"country\"]]\n",
    "\n",
    "    w = edges[\"w\"].to_numpy()\n",
    "    wmax = float(w.max())\n",
    "    # linewidth scaling (tuned to look OK for typical normalized W)\n",
    "    lw = 0.2 + 6.0 * (w / (wmax + 1e-30)) ** 0.75\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # node labels (lightweight)\n",
    "    for c in countries:\n",
    "        x, y = pos_c[c]\n",
    "        ax.plot([x], [y], marker=\"o\", ms=3, color=\"black\")\n",
    "    for p in products:\n",
    "        x, y = pos_p[p]\n",
    "        ax.plot([x], [y], marker=\"o\", ms=3, color=\"black\")\n",
    "\n",
    "    # edges as cubic Beziers through center\n",
    "    for i, row in enumerate(edges.itertuples(index=False)):\n",
    "        c = row.country\n",
    "        p = row.product\n",
    "        x0, y0 = pos_c[c]\n",
    "        x1, y1 = pos_p[p]\n",
    "        # control points closer to center\n",
    "        c0 = np.array([0.35 * x0, 0.35 * y0])\n",
    "        c1 = np.array([0.35 * x1, 0.35 * y1])\n",
    "\n",
    "        verts = [(x0, y0), (c0[0], c0[1]), (c1[0], c1[1]), (x1, y1)]\n",
    "        codes = [Path.MOVETO, Path.CURVE4, Path.CURVE4, Path.CURVE4]\n",
    "        path = Path(verts, codes)\n",
    "        patch = PathPatch(path, facecolor=\"none\", edgecolor=edge_color(row._asdict()), lw=lw[i], alpha=0.55)\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    ax.set_title(title + f\"\\n(top {len(edges)} edges)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_alluvial_bipartite(\n",
    "    W_df: pd.DataFrame,\n",
    "    max_edges: int = 250,\n",
    "    min_edge_mass: float | None = None,\n",
    "    title: str = \"Alluvial (Sankey-style) bipartite flow (filtered)\",\n",
    "):\n",
    "    \"\"\"Alluvial/Sankey-style plot in pure Matplotlib.\n",
    "\n",
    "    Draws stacked nodes on left (countries) and right (products),\n",
    "    with polygon bands for the largest flows.\n",
    "    \"\"\"\n",
    "    edges = (\n",
    "        W_df.stack()\n",
    "        .rename(\"w\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"level_0\": \"country\", \"level_1\": \"product\"})\n",
    "    )\n",
    "    edges = edges[edges[\"w\"] > 0].sort_values(\"w\", ascending=False)\n",
    "    if min_edge_mass is not None:\n",
    "        edges = edges[edges[\"w\"] >= float(min_edge_mass)]\n",
    "    edges = edges.head(max_edges)\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        print(\"No edges to plot after filtering.\")\n",
    "        return\n",
    "\n",
    "    countries = list(pd.Index(edges[\"country\"]).unique())\n",
    "    products = list(pd.Index(edges[\"product\"]).unique())\n",
    "\n",
    "    # total mass per node (restricted to displayed edges)\n",
    "    out_mass = edges.groupby(\"country\")[\"w\"].sum().reindex(countries)\n",
    "    in_mass = edges.groupby(\"product\")[\"w\"].sum().reindex(products)\n",
    "\n",
    "    # normalize heights to 1\n",
    "    out_mass = out_mass / out_mass.sum()\n",
    "    in_mass = in_mass / in_mass.sum()\n",
    "\n",
    "    # vertical packing with padding\n",
    "    pad = 0.01\n",
    "\n",
    "    def pack(masses: pd.Series):\n",
    "        spans = {}\n",
    "        y = 0.0\n",
    "        for k, v in masses.items():\n",
    "            y0 = y\n",
    "            y1 = y + float(v)\n",
    "            spans[k] = [y0, y1]\n",
    "            y = y1 + pad\n",
    "        # rescale to [0,1]\n",
    "        total = y - pad\n",
    "        for k in spans:\n",
    "            spans[k][0] /= total\n",
    "            spans[k][1] /= total\n",
    "        return spans\n",
    "\n",
    "    span_c = pack(out_mass)\n",
    "    span_p = pack(in_mass)\n",
    "\n",
    "    # allocate sub-spans per edge within each node\n",
    "    c_cursor = {c: span_c[c][0] for c in countries}\n",
    "    p_cursor = {p: span_p[p][0] for p in products}\n",
    "\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    c_color = {c: cmap(i % 20) for i, c in enumerate(countries)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11, 7))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    xL, xR = 0.1, 0.9\n",
    "    node_w = 0.03\n",
    "\n",
    "    # draw nodes\n",
    "    for c in countries:\n",
    "        y0, y1 = span_c[c]\n",
    "        ax.add_patch(Polygon([[xL - node_w, y0], [xL, y0], [xL, y1], [xL - node_w, y1]], closed=True, color=\"black\", alpha=0.15))\n",
    "        ax.text(xL - node_w - 0.01, (y0 + y1) / 2, str(c), ha=\"right\", va=\"center\", fontsize=8)\n",
    "\n",
    "    for p in products:\n",
    "        y0, y1 = span_p[p]\n",
    "        ax.add_patch(Polygon([[xR, y0], [xR + node_w, y0], [xR + node_w, y1], [xR, y1]], closed=True, color=\"black\", alpha=0.15))\n",
    "        ax.text(xR + node_w + 0.01, (y0 + y1) / 2, str(p), ha=\"left\", va=\"center\", fontsize=8)\n",
    "\n",
    "    # bands\n",
    "    for row in edges.itertuples(index=False):\n",
    "        c = row.country\n",
    "        p = row.product\n",
    "        w = float(row.w)\n",
    "\n",
    "        # band thickness within each stacked node span (relative to node mass)\n",
    "        dc = w / float(edges[edges[\"country\"] == c][\"w\"].sum()) * (span_c[c][1] - span_c[c][0])\n",
    "        dp = w / float(edges[edges[\"product\"] == p][\"w\"].sum()) * (span_p[p][1] - span_p[p][0])\n",
    "\n",
    "        y0c, y1c = c_cursor[c], c_cursor[c] + dc\n",
    "        y0p, y1p = p_cursor[p], p_cursor[p] + dp\n",
    "        c_cursor[c] = y1c\n",
    "        p_cursor[p] = y1p\n",
    "\n",
    "        # simple 4-point polygon band (looks OK with alpha)\n",
    "        poly = Polygon(\n",
    "            [[xL, y0c], [xR, y0p], [xR, y1p], [xL, y1c]],\n",
    "            closed=True,\n",
    "            facecolor=c_color[c],\n",
    "            edgecolor=\"none\",\n",
    "            alpha=0.45,\n",
    "        )\n",
    "        ax.add_patch(poly)\n",
    "\n",
    "    ax.set_title(title + f\"\\n(top {len(edges)} edges)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_dual_potential_bipartite(\n",
    "    M: pd.DataFrame,\n",
    "    W_df: pd.DataFrame,\n",
    "    u: np.ndarray,\n",
    "    v: np.ndarray,\n",
    "    max_edges: int = 400,\n",
    "    title: str = \"Dual potentials (log u, log v) with flow edges\",\n",
    "):\n",
    "    \"\"\"Layered bipartite plot: node color = dual potentials, edge thickness = w_cp.\"\"\"\n",
    "    countries = list(M.index)\n",
    "    products = list(M.columns)\n",
    "\n",
    "    phi = pd.Series(np.log(u + 1e-30), index=countries)\n",
    "    psi = pd.Series(np.log(v + 1e-30), index=products)\n",
    "\n",
    "    # order by potential for a clean “landscape”\n",
    "    c_order = list(phi.sort_values().index)\n",
    "    p_order = list(psi.sort_values().index)\n",
    "\n",
    "    # pick top edges globally\n",
    "    edges = (\n",
    "        W_df.loc[c_order, p_order]\n",
    "        .stack()\n",
    "        .rename(\"w\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"level_0\": \"country\", \"level_1\": \"product\"})\n",
    "    )\n",
    "    edges = edges[edges[\"w\"] > 0].sort_values(\"w\", ascending=False).head(max_edges)\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        print(\"No edges to plot.\")\n",
    "        return\n",
    "\n",
    "    # positions\n",
    "    y_c = {c: i for i, c in enumerate(c_order)}\n",
    "    y_p = {p: i for i, p in enumerate(p_order)}\n",
    "\n",
    "    x_c, x_p = 0.0, 1.0\n",
    "\n",
    "    # color mapping\n",
    "    vals = np.concatenate([phi.to_numpy(), psi.to_numpy()])\n",
    "    vmin, vmax = np.percentile(vals, [5, 95])\n",
    "    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # edges\n",
    "    w = edges[\"w\"].to_numpy()\n",
    "    wmax = float(w.max())\n",
    "    lw = 0.2 + 4.5 * (w / (wmax + 1e-30)) ** 0.7\n",
    "\n",
    "    for i, row in enumerate(edges.itertuples(index=False)):\n",
    "        c = row.country\n",
    "        p = row.product\n",
    "        ax.plot([x_c, x_p], [y_c[c], y_p[p]], color=\"black\", alpha=0.12, lw=lw[i])\n",
    "\n",
    "    # nodes\n",
    "    ax.scatter([x_c] * len(c_order), [y_c[c] for c in c_order], c=[cmap(norm(phi[c])) for c in c_order], s=18, edgecolor=\"none\")\n",
    "    ax.scatter([x_p] * len(p_order), [y_p[p] for p in p_order], c=[cmap(norm(psi[p])) for p in p_order], s=18, edgecolor=\"none\")\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([x_c, x_p])\n",
    "    ax.set_xticklabels([\"countries\", \"products\"])\n",
    "    ax.set_title(title + f\"\\n(node color = log dual, top {len(edges)} edges)\")\n",
    "\n",
    "    sm = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.02)\n",
    "    cbar.set_label(\"dual potential (log scale)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_ranked_barcodes(\n",
    "    results_countries: pd.DataFrame,\n",
    "    results_products: pd.DataFrame,\n",
    "    top_n: int = 40,\n",
    "    title: str = \"Ranked barcodes (Fitness/Complexity) with degree overlays\",\n",
    "):\n",
    "    \"\"\"Two clean rank plots: countries by Fitness, products by Complexity.\"\"\"\n",
    "    rc = results_countries.sort_values(\"Fitness\", ascending=False).head(top_n)\n",
    "    rp = results_products.sort_values(\"Complexity\", ascending=False).head(top_n)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # countries\n",
    "    ax0 = ax[0]\n",
    "    ax0.bar(range(len(rc)), rc[\"Fitness\"].to_numpy(), color=\"black\", alpha=0.6)\n",
    "    ax0.set_title(f\"Countries (top {len(rc)})\")\n",
    "    ax0.set_xlabel(\"rank\")\n",
    "    ax0.set_ylabel(\"Fitness\")\n",
    "\n",
    "    ax0b = ax0.twinx()\n",
    "    ax0b.plot(range(len(rc)), rc[\"diversification_kc\"].to_numpy(), color=\"tab:blue\", lw=1.5)\n",
    "    ax0b.set_ylabel(\"diversification (kc)\")\n",
    "\n",
    "    # products\n",
    "    ax1 = ax[1]\n",
    "    ax1.bar(range(len(rp)), rp[\"Complexity\"].to_numpy(), color=\"black\", alpha=0.6)\n",
    "    ax1.set_title(f\"Products (top {len(rp)})\")\n",
    "    ax1.set_xlabel(\"rank\")\n",
    "    ax1.set_ylabel(\"Complexity\")\n",
    "\n",
    "    ax1b = ax1.twinx()\n",
    "    ax1b.plot(range(len(rp)), rp[\"ubiquity_kp\"].to_numpy(), color=\"tab:orange\", lw=1.5)\n",
    "    ax1b.set_ylabel(\"ubiquity (kp)\")\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
