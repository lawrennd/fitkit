{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawrennd/fitkit/blob/main/examples/wikipedia_editing_fitness_complexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "## Wikipedia Editing Data: fitness / complexity analysis\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Downloads a sample of Wikipedia users from BigQuery and aggregates their edits into per-user text.\n",
        "- Builds a `user` $\\times$ `word` matrix and its *support* (analogous to `country` $\\times$ `product`).\n",
        "\n",
        "**Expected behavior**: Wikipedia likely has **community/modular structure** (different specialist fields like astrophysics, biology, history) rather than a single nested hierarchy. This means:\n",
        "- **ECI** will show **poor correlation** with diversification globally (community structure breaks its assumption)\n",
        "- **Eigenvalue analysis** can detect and quantify community structure\n",
        "- **Community detection** can separate users into specialist groups\n",
        "- **ECI may work better within communities** (where structure is more nested)\n",
        "- **log(Fitness)** remains **robust** as a global complexity measure\n",
        "- See the nested_matrix notebook for systematic analysis of this effect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b",
      "metadata": {},
      "source": [
        "### Why log(Fitness)?\n",
        "\n",
        "Throughout this notebook, we use **log(Fitness)** for correlations with ECI and diversification because:\n",
        "- Fitness is a **multiplicative/exponential** quantity (spans many orders of magnitude)\n",
        "- ECI and diversification are **linear** scales\n",
        "- log(Fitness) provides the appropriate scale for meaningful comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "### BigQuery Setup Instructions\n",
        "\n",
        "To run the BigQuery query cells in this notebook, you need to have a Google Cloud Project with the BigQuery API enabled and proper authentication setup.\n",
        "\n",
        "Here's a general guide:\n",
        "\n",
        "1.  **Google Cloud Account**: If you don't have one, sign up for a Google Cloud account. You might be eligible for a free trial.\n",
        "    *   [Sign up for Google Cloud](https://cloud.google.com/free)\n",
        "\n",
        "2.  **Create/Select a Project**: In the [Google Cloud Console](https://console.cloud.google.com/), create a new project or select an existing one.\n",
        "    *   Ensure that **billing is enabled** for your project, as BigQuery usage incurs costs (though often minimal for small queries, especially with the free tier).\n",
        "\n",
        "3.  **Enable the BigQuery API**: For your selected project, ensure the BigQuery API is enabled.\n",
        "    *   Go to the [API Library](https://console.cloud.google.com/apis/library) in the Cloud Console.\n",
        "    *   Search for \"BigQuery API\" and enable it if it's not already enabled.\n",
        "\n",
        "4.  **Authentication**:\n",
        "    \n",
        "    **In Google Colab**: Authentication is automatic. The `WikipediaLoader` will detect the Colab environment and use `google.colab.auth.authenticate_user()` to prompt you to log in with your Google account.\n",
        "    \n",
        "    **In Local Jupyter**: You need to set up Application Default Credentials (ADC) using the `gcloud` CLI:\n",
        "    ```bash\n",
        "    gcloud auth application-default login\n",
        "    ```\n",
        "    This will authenticate you and allow the `WikipediaLoader` to access BigQuery using your credentials.\n",
        "\n",
        "Once these steps are complete, you should be able to run the BigQuery cells successfully!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "### 0) Setup\n",
        "\n",
        "You’ll need BigQuery credentials configured locally (e.g. `gcloud auth application-default login`).\n",
        "\n",
        "If you don’t have BigQuery access, you can still run the later cells by loading from the cached parquet file (see the caching cell below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64de1c3e",
      "metadata": {
        "id": "64de1c3e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _pip_install(args: list[str]) -> None:\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", *args]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "def ensure_fitkit_installed() -> None:\n",
        "    \"\"\"Prefer editable local install; fall back to GitHub.\n",
        "\n",
        "    - Local (typical): `pip install -e ..` when running from `examples/`\n",
        "    - Colab/remote: `pip install git+https://github.com/lawrennd/fitkit.git`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import fitkit  # noqa: F401\n",
        "\n",
        "        return\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    here = Path.cwd().resolve()\n",
        "    candidates = [here, here.parent, here.parent.parent]\n",
        "\n",
        "    for root in candidates:\n",
        "        if (root / \"pyproject.toml\").exists() and (root / \"fitkit\").is_dir():\n",
        "            _pip_install([\"install\", \"-e\", str(root)])\n",
        "            return\n",
        "\n",
        "    _pip_install([\"install\", \"git+https://github.com/lawrennd/fitkit.git\"])\n",
        "\n",
        "\n",
        "ensure_fitkit_installed()\n",
        "import fitkit\n",
        "\n",
        "print(\"fitkit version:\", getattr(fitkit, \"__version__\", \"unknown\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9efa654",
      "metadata": {
        "id": "a9efa654"
      },
      "outputs": [],
      "source": [
        "from fitkit.data import WikipediaLoader, QueryConfig, create_small_fixture\n",
        "from fitkit.algorithms import FitnessComplexity, ECI, SinkhornScaler\n",
        "from fitkit.algorithms import fitness_complexity, compute_eci_pci, sinkhorn_masked  # functional API also available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "# Core\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sparse matrices\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "# Standard random sampling (no specific users)\n",
        "cfg = QueryConfig()\n",
        "\n",
        "CACHE_DIR = \"data\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Updated cache path for Wikipedia data (v4 - random sample)\n",
        "CACHE_PATH = os.path.join(\n",
        "    CACHE_DIR,\n",
        "    f\"wikipedia_authors{cfg.max_authors}_v4.parquet\",\n",
        ")\n",
        "\n",
        "print(\"Cache path:\", CACHE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": [],
      "source": [
        "# Load data using WikipediaLoader\n",
        "print(f\"Using cache path: {CACHE_PATH}\")\n",
        "\n",
        "loader = WikipediaLoader(cfg, CACHE_PATH)\n",
        "bundle = loader.load()\n",
        "\n",
        "# Extract components from bundle\n",
        "X = bundle.matrix\n",
        "user_ids = bundle.row_labels.tolist()\n",
        "vocab = bundle.col_labels.tolist()\n",
        "\n",
        "print(f\"Loaded: {len(user_ids)} users, {len(vocab)} words\")\n",
        "print(f\"Matrix shape: {X.shape}, dtype: {X.dtype}\")\n",
        "print(f\"Matrix is sparse: {sp.issparse(X)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "# 2) Extract support matrix and prepare for analysis\n",
        "#\n",
        "# In the paper's language, we will treat the *support* as M_{uw} = 1{X_{uw} > 0}.\n",
        "# The matrix X from WikipediaLoader already has filtering applied (via QueryConfig).\n",
        "# The loader uses binary=False by default (word counts), but we can work with either.\n",
        "\n",
        "# Support mask (structural zeros off-support)\n",
        "M = X.copy()\n",
        "M.data = np.ones_like(M.data)\n",
        "\n",
        "# Basic margins (analogues of diversification and ubiquity)\n",
        "user_strength = np.asarray(X.sum(axis=1)).ravel()\n",
        "word_strength = np.asarray(X.sum(axis=0)).ravel()\n",
        "\n",
        "print(\"User strength:\", pd.Series(user_strength).describe())\n",
        "print(\"Word strength:\", pd.Series(word_strength).describe())\n",
        "print(f\"Matrix -> Users: {X.shape[0]}, Vocab: {X.shape[1]}\")\n",
        "\n",
        "# Labeled view for plotting and downstream helpers\n",
        "M_df = pd.DataFrame.sparse.from_spmatrix(M, index=user_ids, columns=vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "### 3) Baseline: 1D Pietronero Fitness–Complexity fixed point\n",
        "\n",
        "This is the usual nonlinear rank-1 fixed point on the **support matrix** \\(M\\) (binary incidence). We’ll compute it as a scalar reference, then move to the rank-2 extension.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "### Fitness–Complexity ⇄ IPF/Sinkhorn equivalence (what the paper is using)\n",
        "\n",
        "In the paper (`economic-fitness.tex`), the key point is that **Fitness–Complexity is a reparameterisation of masked IPF/Sinkhorn matrix scaling** on the support graph.\n",
        "\n",
        "- We solve for a coupling/flow \\(w_{uw}\\ge 0\\) supported on \\(M\\) such that \\(\\sum_w w_{uw}=r_u\\) and \\(\\sum_u w_{uw}=c_w\\).\n",
        "- IPF/Sinkhorn gives a diagonal scaling solution \\(w_{uw} = M_{uw} A_u B_w\\).\n",
        "- Setting \\(A_u \\equiv 1/F_u\\) and \\(B_w \\equiv Q_w\\) yields \\(w_{uw} \\propto M_{uw} Q_w/F_u\\), and the FC fixed-point updates recover the scaling equations (up to the usual projective normalisation/gauge).\n",
        "\n",
        "So the Sinkhorn/IPF object here is **not a different model**—it’s the same masked matrix-scaling problem, viewed in “flow” form. The only extra modelling choice is **which marginals \\((r,c)\\)** to impose (uniform is a common default in the support-only setting; data-marginals are natural for quantitative flows).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "# Use sklearn-style estimators\n",
        "fc = FitnessComplexity()\n",
        "F, Q = fc.fit_transform(M)\n",
        "fc_hist = fc.history_\n",
        "\n",
        "eci_model = ECI()\n",
        "eci, pci = eci_model.fit_transform(M)\n",
        "\n",
        "F_s = pd.Series(F, index=user_ids, name=\"Fitness\")\n",
        "Q_s = pd.Series(Q, index=vocab, name=\"Complexity\")\n",
        "eci_s = pd.Series(eci, index=user_ids, name=\"ECI\")\n",
        "pci_s = pd.Series(pci, index=vocab, name=\"PCI\")\n",
        "\n",
        "kc = pd.Series(np.asarray(M.sum(axis=1)).ravel(), index=user_ids, name=\"diversification_kc\")\n",
        "kp = pd.Series(np.asarray(M.sum(axis=0)).ravel(), index=vocab, name=\"ubiquity_kp\")\n",
        "\n",
        "# Sinkhorn/IPF scaling to build a flow W on the support.\n",
        "# For the FC ⇄ Sinkhorn equivalence viewpoint, the natural default is *uniform* marginals.\n",
        "# However, uniform marginals can be infeasible on some sparse masks; we fall back if needed.\n",
        "\n",
        "# default: uniform marginals (same total mass, different per-node mass if rectangular)\n",
        "r_uniform = np.ones(M.shape[0], dtype=float)\n",
        "r_uniform = r_uniform / r_uniform.sum()\n",
        "c_uniform = np.ones(M.shape[1], dtype=float)\n",
        "c_uniform = c_uniform / c_uniform.sum()\n",
        "\n",
        "scaler = SinkhornScaler()\n",
        "W = scaler.fit_transform(M, row_marginals=r_uniform, col_marginals=c_uniform)\n",
        "u, v, sk_hist = scaler.u_, scaler.v_, scaler.history_\n",
        "\n",
        "if not sk_hist.get(\"converged\", False):\n",
        "    print(\"Sinkhorn with uniform marginals did not converge; falling back to degree marginals.\")\n",
        "    r_deg = kc.to_numpy(dtype=float)\n",
        "    r_deg = r_deg / r_deg.sum()\n",
        "    c_deg = kp.to_numpy(dtype=float)\n",
        "    c_deg = c_deg / c_deg.sum()\n",
        "    scaler = SinkhornScaler()\n",
        "    W = scaler.fit_transform(M, row_marginals=r_deg, col_marginals=c_deg)\n",
        "    u, v, sk_hist = scaler.u_, scaler.v_, scaler.history_\n",
        "\n",
        "results_countries = pd.concat([F_s, eci_s, kc], axis=1).sort_values(\"Fitness\", ascending=False)\n",
        "results_products = pd.concat([Q_s, pci_s, kp], axis=1).sort_values(\"Complexity\", ascending=False)\n",
        "\n",
        "word_scores_1d = Q_s.sort_values(ascending=False)\n",
        "user_scores_1d = F_s.sort_values(ascending=False)\n",
        "\n",
        "print(\"Top 20 words by complexity:\")\n",
        "print(word_scores_1d.head(20))\n",
        "print(\"Top 20 users by fitness:\")\n",
        "print(user_scores_1d.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "## Flow-native visualisations (Sinkhorn/OT coupling) + ranked barcodes\n",
        "\n",
        "The objects we visualise here are:\n",
        "\n",
        "- binary support: `M` (country×product)\n",
        "- Sinkhorn/IPF scaling factors: `u`, `v` (dual variables)\n",
        "- coupling / feasible flow: `W` where `W = diag(u) * M * diag(v)` (on the support)\n",
        "\n",
        "To avoid “hairballs”, every flow plot below supports **top-k / top-edge filtering**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "# Diagnostics: convergence\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
        "ax[0].plot(fc.history_[\"dF\"], label=\"max |ΔF|\")\n",
        "ax[0].plot(fc.history_[\"dQ\"], label=\"max |ΔQ|\")\n",
        "ax[0].set_yscale(\"log\")\n",
        "ax[0].set_title(\"FC convergence\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(scaler.history_[\"dr\"], label=\"max row marginal error\")\n",
        "ax[1].plot(scaler.history_[\"dc\"], label=\"max col marginal error\")\n",
        "ax[1].set_yscale(\"log\")\n",
        "ax[1].set_title(\"Sinkhorn/IPF convergence\")\n",
        "ax[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Diagnostics: nestedness-like visualization (sort by Fitness/Complexity)\n",
        "M_sorted = M_df.loc[results_countries.index, results_products.index]\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.imshow(M_sorted.sparse.to_dense().to_numpy(), aspect=\"auto\", interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.title(\"M sorted by Fitness (rows) and Complexity (cols)\")\n",
        "plt.xlabel(\"words\")\n",
        "plt.ylabel(\"users\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Diagnostics: compare rankings\n",
        "# Calculate log(Fitness) for meaningful correlation with linear scales\n",
        "results_countries[\"log_Fitness\"] = np.log(results_countries[\"Fitness\"])\n",
        "\n",
        "# Compute correlations\n",
        "corr_eci_div = results_countries[\"ECI\"].corr(results_countries[\"diversification_kc\"])\n",
        "corr_logF_div = results_countries[\"log_Fitness\"].corr(results_countries[\"diversification_kc\"])\n",
        "corr_eci_logF = results_countries[\"ECI\"].corr(results_countries[\"log_Fitness\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Correlation(ECI, Diversification):         {corr_eci_div:.4f}\")\n",
        "print(f\"Correlation(log(Fitness), Diversification): {corr_logF_div:.4f}\")\n",
        "print(f\"Correlation(ECI, log(Fitness)):             {corr_eci_logF:.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Low ECI correlations suggest Wikipedia has COMMUNITY/MODULAR structure\")\n",
        "print(\"- Different specialist communities (astrophysics, biology, history, etc.)\")\n",
        "print(\"  each have their own specialized vocabulary (rare technical terms)\")\n",
        "print(\"- This breaks the single nested hierarchy that ECI requires\")\n",
        "print(\"- log(Fitness) remains robust: works across different data structures\")\n",
        "print(\"\\nWhy ECI fails here:\")\n",
        "print(\"- ECI assumes: high-diversity users use ALL words low-diversity users have\")\n",
        "print(\"- Reality: specialist editors use rare technical terms generalists don't\")\n",
        "print(\"- Just 2 communities drops ECI correlation from 0.9 → 0.1 (see nested notebook)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(results_countries[\"ECI\"], results_countries[\"Fitness\"], s=15, alpha=0.7)\n",
        "plt.xlabel(\"ECI (standardised)\")\n",
        "plt.ylabel(\"Fitness\")\n",
        "plt.title(f\"Users: Fitness vs ECI\\n(Correlation with log(Fitness): {corr_eci_logF:.3f})\")\n",
        "plt.yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358326e8",
      "metadata": {},
      "source": [
        "### Eigenvalue Spectrum Analysis\n",
        "\n",
        "**Key diagnostic**: The eigenvalue spectrum of the country-country projection matrix C reveals community structure.\n",
        "\n",
        "**Expected patterns**:\n",
        "- **Perfect nesting**: λ₂ >> λ₃ >> ... (clear spectral gap, single dominant structure)\n",
        "- **k Communities**: λ₂ ≈ λ₃ ≈ ... ≈ λₖ (multiple significant eigenvalues)\n",
        "\n",
        "The **spectral gap ratio** (λ₂/λ₃) tells us:\n",
        "- Large ratio (>5): Single nested hierarchy → ECI works\n",
        "- Small ratio (<2): Multiple communities → ECI fails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539c7699",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute country-country projection matrix (what ECI uses)\n",
        "# C = (M/kc) @ (M^T/kp)\n",
        "kc = np.asarray(M.sum(axis=1)).ravel()  # diversification\n",
        "kp = np.asarray(M.sum(axis=0)).ravel()  # ubiquity\n",
        "\n",
        "# Avoid division by zero\n",
        "kc_safe = np.where(kc > 0, kc, 1)\n",
        "kp_safe = np.where(kp > 0, kp, 1)\n",
        "\n",
        "# Compute C\n",
        "M_normalized = M.toarray() / kc_safe[:, np.newaxis]\n",
        "M_T_normalized = M.toarray().T / kp_safe[:, np.newaxis]\n",
        "C = M_normalized @ M_T_normalized\n",
        "\n",
        "# Compute eigenvalues\n",
        "eigenvalues = np.linalg.eigvalsh(C)\n",
        "eigenvalues = np.sort(eigenvalues)[::-1]  # Sort descending\n",
        "\n",
        "# Analyze top eigenvalues\n",
        "n_show = min(15, len(eigenvalues))\n",
        "top_eigenvalues = eigenvalues[:n_show]\n",
        "\n",
        "# Compute spectral gap\n",
        "spectral_gap = eigenvalues[1] / eigenvalues[2] if len(eigenvalues) > 2 else np.inf\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EIGENVALUE SPECTRUM ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTop {n_show} eigenvalues of country-country matrix C:\")\n",
        "print(\"-\" * 70)\n",
        "for i, eig in enumerate(top_eigenvalues, 1):\n",
        "    marker = \"  ← ECI uses this\" if i == 2 else \"\"\n",
        "    print(f\"  λ{i:<2} = {eig:>8.4f}{marker}\")\n",
        "\n",
        "print(f\"\\nSpectral gap ratio (λ₂/λ₃): {spectral_gap:.2f}\")\n",
        "\n",
        "# Compute relative magnitudes\n",
        "if len(eigenvalues) > 3:\n",
        "    rel_3 = eigenvalues[2] / eigenvalues[1]\n",
        "    rel_4 = eigenvalues[3] / eigenvalues[1]\n",
        "    print(f\"Relative magnitudes: λ₃/λ₂={rel_3:.2f}, λ₄/λ₂={rel_4:.2f}\")\n",
        "\n",
        "# Count significant eigenvalues (>20% of λ₂)\n",
        "n_sig = np.sum(eigenvalues > 0.2 * eigenvalues[1])\n",
        "print(f\"\\nSignificant eigenvalues (>20% of λ₂): {n_sig}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "if spectral_gap > 5:\n",
        "    print(\"  ✓ Large spectral gap → Single dominant structure → ECI should work\")\n",
        "elif spectral_gap > 2:\n",
        "    print(\"  ⚠ Moderate spectral gap → Mixed structure → ECI may struggle\")\n",
        "else:\n",
        "    print(\"  ✗ Small spectral gap → Multiple communities → ECI fails\")\n",
        "    print(f\"    {n_sig} significant eigenvalues → {n_sig-1} communities likely present\")\n",
        "\n",
        "print(\"\\n⚠️  KEY INSIGHT:\")\n",
        "print(\"  ECI uses only λ₂ (2nd eigenvector = 1 dimension) to capture complexity\")\n",
        "print(f\"  But with {n_sig} significant eigenvalues, we need {n_sig-1} dimensions (λ₂, λ₃, ...)\")\n",
        "print(\"  The single-dimension projection misses most of the structure!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5266af30",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize eigenvalue spectrum\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Top eigenvalues (linear scale)\n",
        "ax1.bar(range(1, n_show + 1), top_eigenvalues, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax1.axvline(2, color='red', linestyle='--', linewidth=2, label='ECI uses λ₂', alpha=0.7)\n",
        "ax1.set_xlabel('Eigenvalue Index', fontsize=12)\n",
        "ax1.set_ylabel('Eigenvalue', fontsize=12)\n",
        "ax1.set_title('Top Eigenvalues of Country-Country Matrix', fontsize=13)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Eigenvalue decay (log scale)\n",
        "ax2.semilogy(range(1, n_show + 1), top_eigenvalues, 'o-', markersize=8, linewidth=2, color='steelblue')\n",
        "ax2.axvline(2, color='red', linestyle='--', linewidth=2, label='ECI uses λ₂', alpha=0.7)\n",
        "ax2.axhline(0.1 * top_eigenvalues[1], color='orange', linestyle=':', linewidth=2, \n",
        "            label='10% of λ₂ threshold', alpha=0.7)\n",
        "ax2.set_xlabel('Eigenvalue Index', fontsize=12)\n",
        "ax2.set_ylabel('Eigenvalue (log scale)', fontsize=12)\n",
        "ax2.set_title('Eigenvalue Decay (reveals community structure)', fontsize=13)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ If flat eigenvalue spectrum → Multiple communities → ECI fails\")\n",
        "print(\"✓ If exponential decay → Single hierarchy → ECI works\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "community_detection_header",
      "metadata": {},
      "source": [
        "## Community Detection via Eigenvector Clustering\n",
        "\n",
        "Since the eigenvalue spectrum reveals multiple communities, we can:\n",
        "1. Project each user onto the top eigenvectors\n",
        "2. Cluster users based on which eigenvector(s) they align with\n",
        "3. Run ECI *within* each community (where it should work better)\n",
        "4. Maintain Fitness as the global complexity measure\n",
        "\n",
        "**Key Insight**: ECI fails globally because it uses only λ₂ (1 dimension), but by separating communities first, ECI can work within each community's nested structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "community_detection_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Compute eigenvectors (not just eigenvalues)\n",
        "eigenvalues_full, eigenvectors = np.linalg.eigh(C)\n",
        "# Sort by eigenvalue (descending)\n",
        "idx = np.argsort(eigenvalues_full)[::-1]\n",
        "eigenvalues_full = eigenvalues_full[idx]\n",
        "eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "# Project users onto top k eigenvectors (excluding λ₁ which is trivial uniform)\n",
        "# We use 2-5 eigenvectors to capture the community structure\n",
        "n_components = min(5, eigenvectors.shape[1])\n",
        "user_projections = eigenvectors[:, 1:n_components+1]  # Skip λ₁\n",
        "\n",
        "print(f\"User projections shape: {user_projections.shape}\")\n",
        "print(f\"Using top {n_components} eigenvectors (λ₂ through λ_{n_components+1})\")\n",
        "\n",
        "# Determine optimal number of communities\n",
        "# Use the number of significant eigenvalues as a guide\n",
        "n_sig = np.sum(eigenvalues_full > 0.2 * eigenvalues_full[1])\n",
        "n_communities = min(n_sig - 1, 5)  # -1 because λ₁ is trivial, cap at 5 for interpretability\n",
        "\n",
        "print(f\"\\nDetected {n_sig} significant eigenvalues\")\n",
        "print(f\"Clustering into {n_communities} communities\\n\")\n",
        "\n",
        "# Standardize projections for clustering\n",
        "scaler = StandardScaler()\n",
        "user_projections_scaled = scaler.fit_transform(user_projections)\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=n_communities, random_state=42, n_init=20)\n",
        "community_labels = kmeans.fit_predict(user_projections_scaled)\n",
        "\n",
        "# Add community labels to results\n",
        "results_countries['community'] = community_labels\n",
        "\n",
        "# Print community statistics\n",
        "print(\"=\"*70)\n",
        "print(\"COMMUNITY STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "for comm in range(n_communities):\n",
        "    mask = community_labels == comm\n",
        "    n_users = mask.sum()\n",
        "    avg_div = results_countries.loc[mask, 'diversification_kc'].mean()\n",
        "    avg_fitness = results_countries.loc[mask, 'Fitness'].mean()\n",
        "    print(f\"\\nCommunity {comm}:\")\n",
        "    print(f\"  Users: {n_users} ({100*n_users/len(results_countries):.1f}%)\")\n",
        "    print(f\"  Avg diversification: {avg_div:.1f}\")\n",
        "    print(f\"  Avg Fitness: {avg_fitness:.3f}\")\n",
        "    # Show top 3 users\n",
        "    top_users = results_countries.loc[mask].nlargest(3, 'Fitness')\n",
        "    print(f\"  Top users: {', '.join(top_users.index[:3])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "community_visualization",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize communities in eigenvector space\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Users projected onto λ₂ vs λ₃\n",
        "ax1 = axes[0]\n",
        "for comm in range(n_communities):\n",
        "    mask = community_labels == comm\n",
        "    ax1.scatter(user_projections[mask, 0], user_projections[mask, 1], \n",
        "                label=f'Community {comm}', alpha=0.6, s=50)\n",
        "ax1.set_xlabel('λ₂ (2nd eigenvector)', fontsize=12)\n",
        "ax1.set_ylabel('λ₃ (3rd eigenvector)', fontsize=12)\n",
        "ax1.set_title('Users Clustered by Eigenvector Projections', fontsize=13)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Community sizes and average Fitness\n",
        "ax2 = axes[1]\n",
        "comm_sizes = [np.sum(community_labels == c) for c in range(n_communities)]\n",
        "comm_fitness = [results_countries.loc[community_labels == c, 'Fitness'].mean() \n",
        "                for c in range(n_communities)]\n",
        "x = np.arange(n_communities)\n",
        "width = 0.35\n",
        "ax2_twin = ax2.twinx()\n",
        "bars1 = ax2.bar(x - width/2, comm_sizes, width, label='# Users', color='steelblue', alpha=0.7)\n",
        "bars2 = ax2_twin.bar(x + width/2, comm_fitness, width, label='Avg Fitness', color='coral', alpha=0.7)\n",
        "ax2.set_xlabel('Community', fontsize=12)\n",
        "ax2.set_ylabel('Number of Users', fontsize=12, color='steelblue')\n",
        "ax2_twin.set_ylabel('Average Fitness', fontsize=12, color='coral')\n",
        "ax2.set_title('Community Sizes and Fitness', fontsize=13)\n",
        "ax2.set_xticks(x)\n",
        "ax2.tick_params(axis='y', labelcolor='steelblue')\n",
        "ax2_twin.tick_params(axis='y', labelcolor='coral')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "community_eci_analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ECI performance within each community vs. globally\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ECI PERFORMANCE: GLOBAL vs. WITHIN-COMMUNITY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Global ECI correlation (we already computed this)\n",
        "print(f\"\\nGlobal ECI correlation with log(Fitness): {corr_eci_logF:.3f}\")\n",
        "print(f\"Global ECI correlation with diversification: {corr_eci_div:.3f}\")\n",
        "\n",
        "print(\"\\nWithin-community ECI correlations:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "within_comm_correlations = []\n",
        "for comm in range(n_communities):\n",
        "    mask = community_labels == comm\n",
        "    comm_data = results_countries.loc[mask]\n",
        "    \n",
        "    # Remove NaN values for correlation\n",
        "    valid_mask = ~np.isnan(comm_data['ECI'])\n",
        "    if valid_mask.sum() > 3:  # Need at least a few points\n",
        "        corr_eci_logF_comm = np.corrcoef(\n",
        "            comm_data.loc[valid_mask, 'ECI'],\n",
        "            comm_data.loc[valid_mask, 'log_Fitness']\n",
        "        )[0, 1]\n",
        "        corr_eci_div_comm = np.corrcoef(\n",
        "            comm_data.loc[valid_mask, 'ECI'],\n",
        "            comm_data.loc[valid_mask, 'diversification_kc']\n",
        "        )[0, 1]\n",
        "        \n",
        "        within_comm_correlations.append({\n",
        "            'community': comm,\n",
        "            'n_users': mask.sum(),\n",
        "            'corr_eci_logF': corr_eci_logF_comm,\n",
        "            'corr_eci_div': corr_eci_div_comm\n",
        "        })\n",
        "        \n",
        "        print(f\"\\nCommunity {comm} (n={mask.sum()}):\")\n",
        "        print(f\"  ECI vs log(Fitness): {corr_eci_logF_comm:6.3f}  {'✓' if abs(corr_eci_logF_comm) > 0.7 else '✗'}\")\n",
        "        print(f\"  ECI vs diversification: {corr_eci_div_comm:6.3f}  {'✓' if abs(corr_eci_div_comm) > 0.7 else '✗'}\")\n",
        "    else:\n",
        "        print(f\"\\nCommunity {comm}: Too few users ({mask.sum()}) for correlation\")\n",
        "\n",
        "# Compute average within-community correlation\n",
        "if within_comm_correlations:\n",
        "    avg_within_corr_logF = np.mean([c['corr_eci_logF'] for c in within_comm_correlations])\n",
        "    avg_within_corr_div = np.mean([c['corr_eci_div'] for c in within_comm_correlations])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY:\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Global ECI correlation with log(Fitness):      {corr_eci_logF:6.3f}\")\n",
        "    print(f\"Average within-community correlation:          {avg_within_corr_logF:6.3f}\")\n",
        "    print(f\"\\nImprovement: {avg_within_corr_logF - corr_eci_logF:+.3f}\")\n",
        "    \n",
        "    print(\"\\n⚠️  KEY FINDING:\")\n",
        "    if avg_within_corr_logF > corr_eci_logF + 0.1:\n",
        "        print(\"  ✓ ECI works BETTER within communities!\")\n",
        "        print(\"  → Community detection + within-community ECI is a viable strategy\")\n",
        "        print(\"  → Use Fitness for global ranking, ECI for within-community ranking\")\n",
        "    else:\n",
        "        print(\"  ✗ ECI still struggles even within communities\")\n",
        "        print(\"  → May need finer community detection or different approach\")\n",
        "        print(\"  → Fitness remains the more robust global measure\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k81TRoZ5ij4F",
      "metadata": {
        "id": "k81TRoZ5ij4F"
      },
      "outputs": [],
      "source": [
        "from fitkit.diagnostics import (\n",
        "    plot_circular_bipartite_flow,\n",
        "    plot_alluvial_bipartite,\n",
        "    plot_dual_potential_bipartite,\n",
        "    plot_ranked_barcodes,\n",
        "    _to_flow_df,\n",
        "    _top_subset,\n",
        ")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "# Prepare data with URL\n",
        "plot_df = results_countries.copy()\n",
        "# Construct Wikipedia User URLs (replacing spaces with underscores)\n",
        "plot_df[\"wiki_url\"] = \"https://en.wikipedia.org/wiki/User:\" + plot_df.index.astype(str).str.replace(' ', '_')\n",
        "\n",
        "# Interactive scatter plot with custom_data for the URL\n",
        "fig = px.scatter(\n",
        "    plot_df,\n",
        "    x=\"ECI\",\n",
        "    y=\"Fitness\",\n",
        "    hover_name=plot_df.index,\n",
        "    hover_data=[\"diversification_kc\", \"log_Fitness\"],\n",
        "    custom_data=[\"wiki_url\"],\n",
        "    title=f\"Users: Fitness vs ECI (Click dot to open User Page)<br>Correlation(ECI, log(Fitness)): {corr_eci_logF:.3f}\",\n",
        "    labels={\"ECI\": \"ECI (standardised)\", \"Fitness\": \"Fitness (log scale)\"},\n",
        "    template=\"plotly_white\",\n",
        "    opacity=0.7,\n",
        "    log_y=True\n",
        ")\n",
        "\n",
        "fig.update_traces(marker=dict(size=8))\n",
        "fig.update_layout(width=700, height=500)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": [],
      "source": [
        "# Plotting functions have been moved to fitkit.diagnostics\n",
        "# Import them from the module instead (see cell above)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "outputs": [],
      "source": [
        "# Build a labeled coupling DataFrame\n",
        "W_df = _to_flow_df(M_df, W)\n",
        "\n",
        "# Sort according to Fitness/Complexity orderings\n",
        "W_sorted = W_df.loc[results_countries.index, results_products.index]\n",
        "\n",
        "# Filter to top nodes for readability\n",
        "W_small = _top_subset(W_sorted, top_c=18, top_p=28, by=\"fitness_complexity\", F_s=F_s, Q_s=Q_s)\n",
        "\n",
        "# 1) Circular bipartite flow (chord-style)\n",
        "plot_circular_bipartite_flow(\n",
        "    W_small,\n",
        "    max_edges=320,\n",
        "    color_by=\"country\",\n",
        "    title=\"Circular bipartite flow for Sinkhorn coupling W (filtered)\",\n",
        ")\n",
        "\n",
        "# 2) Alluvial / Sankey-style flow\n",
        "plot_alluvial_bipartite(\n",
        "    W_small,\n",
        "    max_edges=220,\n",
        "    title=\"Alluvial view of Sinkhorn coupling W (filtered)\",\n",
        ")\n",
        "\n",
        "# 3) Dual potentials landscape (log u/log v) + top edges\n",
        "plot_dual_potential_bipartite(\n",
        "    M=M_df,\n",
        "    W_df=W_df,\n",
        "    u=u,\n",
        "    v=v,\n",
        "    max_edges=450,\n",
        "    title=\"Dual potentials (log u, log v) + flow edges from W\",\n",
        ")\n",
        "\n",
        "# 4) Ranked barcode plots\n",
        "plot_ranked_barcodes(results_countries, results_products, top_n=40)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3904e239",
      "metadata": {
        "id": "3904e239"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ba7ff9ca",
      "metadata": {
        "id": "ba7ff9ca"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "942eccd4",
      "metadata": {
        "id": "942eccd4"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
